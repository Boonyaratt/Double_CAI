{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "456f3b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ML\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5e9ba806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5140\\853814542.py:4: UserWarning: Parsing dates in %d/%m/%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  promotions = pd.read_csv(BASE/\"promotions.csv\", parse_dates=[\"start_date\",\"end_date\"])\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5140\\853814542.py:4: UserWarning: Parsing dates in %d/%m/%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  promotions = pd.read_csv(BASE/\"promotions.csv\", parse_dates=[\"start_date\",\"end_date\"])\n"
     ]
    }
   ],
   "source": [
    "BASE = Path(\"Datasets/mockup_ver2/\")\n",
    "\n",
    "tx_merge = pd.read_csv(BASE/\"tx_merge3.csv\") \n",
    "promotions = pd.read_csv(BASE/\"promotions.csv\", parse_dates=[\"start_date\",\"end_date\"])\n",
    "\n",
    "promos_df = promotions.copy()\n",
    "df = tx_merge.copy()\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "515fe105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "HAS_LGB = True\n",
    "\n",
    "SEED = 42\n",
    "NEED_K = 8\n",
    "PCA_K  = 30\n",
    "TOPK_TYPES = 2\n",
    "REL_TH = 0.30\n",
    "MAX_CANDS = 40\n",
    "\n",
    "# ---- Columns (อิงจาก tx_merge2.csv ของคุณ) ----\n",
    "COL_TX   = \"transaction_id\"\n",
    "COL_USER = \"user_id\"\n",
    "COL_PROD = \"product_id\"\n",
    "COL_QTY  = \"qty\"\n",
    "COL_PRICE= \"price\"\n",
    "\n",
    "COL_CAT   = \"products.category\"\n",
    "COL_BRAND = \"products.brand\"\n",
    "COL_TS    = \"timestamp\"\n",
    "COL_STORE = \"store_id\"\n",
    "COL_ONLINE= \"is_online\"\n",
    "\n",
    "COL_ORDER_H = \"order_hour\"\n",
    "COL_DOW     = \"dayofweek\"\n",
    "COL_MONTH   = \"month\"\n",
    "COL_DAY     = \"day\"\n",
    "COL_WOY     = \"weekofyear\"\n",
    "COL_QUARTER = \"quarter\"\n",
    "COL_IS_WKD  = \"is_weekend\"\n",
    "COL_THAI_SEAS = \"thai_season\"\n",
    "COL_IN_FEST   = \"InFestival\"\n",
    "\n",
    "COL_WKD_BOOST = \"weekday_boost\"\n",
    "COL_WKE_BOOST = \"weekend_boost\"\n",
    "COL_FES_BOOST = \"festival_boost\"\n",
    "COL_PEAKS     = \"peaks_encoded\"\n",
    "COL_HOUR_W    = \"hour_weight\"\n",
    "COL_LOYALTY   = \"loyalty_score\"\n",
    "COL_EXPECT    = \"expected_basket_items\"\n",
    "COL_ELAS      = \"price_elasticity\"\n",
    "COL_SEGMENT   = \"segment\"\n",
    "\n",
    "# ถ้าใช้ label จาก tx_merge โดยตรง:\n",
    "LABEL_COL_IN_TX = \"promotion_type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d4521c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_map = {}\n",
    "if \"promotions.promo_type\" in promos_df.columns:\n",
    "    rename_map[\"promotions.promo_type\"] = \"promo_type\"\n",
    "if \"promotion_category\" in promos_df.columns and \"promo_type\" not in promos_df.columns:\n",
    "    rename_map[\"promotion_category\"] = \"promo_type\"\n",
    "if \"promotion_type\" in promos_df.columns and \"promo_type\" not in promos_df.columns:\n",
    "    rename_map[\"promotion_type\"] = \"promo_type\"\n",
    "if \"scope\" in promos_df.columns and \"product_scope\" not in promos_df.columns:\n",
    "    rename_map[\"scope\"] = \"product_scope\"\n",
    "\n",
    "promos_df = promos_df.rename(columns=rename_map)\n",
    "\n",
    "# เติมคอลัมน์ที่ขาดด้วยค่า default ปลอดภัย\n",
    "defaults = {\n",
    "    \"promo_id\": \"__UNK__\",\n",
    "    \"promo_type\": \"Unknown\",\n",
    "    \"product_scope\": \"\",\n",
    "    \"is_online\": 1,\n",
    "    \"start_date\": pd.Timestamp(\"2000-01-01\"),\n",
    "    \"end_date\":   pd.Timestamp(\"2100-01-01\"),\n",
    "    \"est_margin\": 0.0\n",
    "}\n",
    "for c, d in defaults.items():\n",
    "    if c not in promos_df.columns:\n",
    "        promos_df[c] = d\n",
    "\n",
    "# final check\n",
    "need_cols = [\"promo_id\",\"promo_type\",\"product_scope\",\"is_online\",\"start_date\",\"end_date\",\"est_margin\"]\n",
    "missing = [c for c in need_cols if c not in promos_df.columns]\n",
    "assert not missing, f\"promos_df ขาดคอลัมน์: {missing}\"\n",
    "\n",
    "# แปลงวันที่ (กัน type ผิด)\n",
    "promos_df[\"start_date\"] = pd.to_datetime(promos_df[\"start_date\"], errors=\"coerce\")\n",
    "promos_df[\"end_date\"]   = pd.to_datetime(promos_df[\"end_date\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "672c192b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basket_feat shape: (19178, 85)\n",
      "num FEATURES: 81\n"
     ]
    }
   ],
   "source": [
    "agg = {}\n",
    "if COL_PROD in df.columns: agg[COL_PROD] = \"nunique\"\n",
    "if COL_QTY  in df.columns: agg[COL_QTY]  = \"sum\"\n",
    "if COL_PRICE in df.columns and COL_QTY in df.columns:\n",
    "    df[\"_revenue\"] = df[COL_PRICE].fillna(0) * df[COL_QTY].fillna(0)\n",
    "    agg[\"_revenue\"] = \"sum\"\n",
    "elif COL_PRICE in df.columns:\n",
    "    agg[COL_PRICE] = \"sum\"\n",
    "\n",
    "basket = (\n",
    "    df.groupby(COL_TX).agg(agg)\n",
    "      .rename(columns={COL_PROD: \"basket_unique_items\"})\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "evt = df.groupby(COL_TX)[COL_TS].min().rename(\"event_time\").reset_index()\n",
    "basket = basket.merge(evt, on=COL_TX, how=\"left\")\n",
    "\n",
    "# context ที่มีอยู่แล้วในไฟล์\n",
    "context_cols = [\n",
    "    COL_STORE, COL_ONLINE,\n",
    "    COL_ORDER_H, COL_DOW, COL_MONTH, COL_DAY, COL_WOY, COL_QUARTER,\n",
    "    COL_IS_WKD, COL_THAI_SEAS, COL_IN_FEST,\n",
    "    COL_WKD_BOOST, COL_WKE_BOOST, COL_FES_BOOST, COL_PEAKS, COL_HOUR_W,\n",
    "    COL_LOYALTY, COL_EXPECT, COL_ELAS, COL_SEGMENT\n",
    "]\n",
    "\n",
    "for c in context_cols:\n",
    "    if c in df.columns:\n",
    "        first = df.groupby(COL_TX)[c].first().reset_index()\n",
    "        basket = basket.merge(first, on=COL_TX, how=\"left\")\n",
    "\n",
    "# multi-hot: k=category/brand proportions\n",
    "def crosstab_prop(frame, key, val, prefix):\n",
    "    if val not in frame.columns:\n",
    "        return pd.DataFrame({key: frame[key].unique()})\n",
    "    ct = pd.crosstab(frame[key], frame[val])\n",
    "    if ct.empty:\n",
    "        return pd.DataFrame({key: frame[key].unique()})\n",
    "    prop = ct.div(ct.sum(axis=1).replace(0, np.nan), axis=0).fillna(0)\n",
    "    prop.columns = [f\"{prefix}={c}\" for c in prop.columns]\n",
    "    return prop.reset_index()\n",
    "\n",
    "cat_prop   = crosstab_prop(df, COL_TX, COL_CAT,   \"cat\")\n",
    "brand_prop = crosstab_prop(df, COL_TX, COL_BRAND, \"brand\")\n",
    "basket = basket.merge(cat_prop, on=COL_TX, how=\"left\").merge(brand_prop, on=COL_TX, how=\"left\")\n",
    "\n",
    "if COL_ONLINE in basket.columns:\n",
    "    basket[COL_ONLINE] = basket[COL_ONLINE].astype(int)\n",
    "\n",
    "comp_cols = [c for c in basket.columns if c.startswith(\"cat=\") or c.startswith(\"brand=\")]\n",
    "num_cols = [\n",
    "    \"basket_unique_items\", COL_QTY, \"_revenue\", COL_PRICE,\n",
    "    COL_ORDER_H, COL_DOW, COL_MONTH, COL_DAY, COL_WOY, COL_QUARTER,\n",
    "    COL_IS_WKD, COL_THAI_SEAS, COL_IN_FEST, COL_WKD_BOOST, COL_WKE_BOOST, COL_FES_BOOST,\n",
    "    COL_PEAKS, COL_HOUR_W, COL_LOYALTY, COL_EXPECT, COL_ELAS\n",
    "]\n",
    "num_cols = [c for c in num_cols if c in basket.columns]\n",
    "\n",
    "FEATURE_COLS = num_cols + ([COL_ONLINE] if COL_ONLINE in basket.columns else []) + comp_cols\n",
    "basket_feat = basket.copy()\n",
    "\n",
    "# sanity print\n",
    "print(\"basket_feat shape:\", basket_feat.shape)\n",
    "print(\"num FEATURES:\", len(FEATURE_COLS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0ed17d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_types(probs, classes, k=2, ensure_non_nopromo=2, nopromo_label=\"NoPromo\"):\n",
    "    \"\"\"\n",
    "    เลือกประเภทโปรฯ สำหรับ recall: บังคับให้มีอย่างน้อย ensure_non_nopromo ประเภทที่ไม่ใช่ NoPromo\n",
    "    แล้วค่อยเติม NoPromo ในลิสต์ (ถ้าจำเป็น)\n",
    "    \"\"\"\n",
    "    order = np.argsort(probs)[::-1]\n",
    "    cls_order = [classes[i] for i in order]\n",
    "\n",
    "    non_np = [c for c in cls_order if c != nopromo_label]\n",
    "    top_non_np = non_np[:max(ensure_non_nopromo, 1)]\n",
    "\n",
    "    merged, seen = [], set()\n",
    "    for c in top_non_np + cls_order:\n",
    "        if c not in seen:\n",
    "            merged.append(c); seen.add(c)\n",
    "        if len(merged) >= k + 1:  # เผื่อ 1 ช่องให้ NoPromo\n",
    "            break\n",
    "\n",
    "    if nopromo_label not in merged:\n",
    "        merged.append(nopromo_label)\n",
    "\n",
    "    return merged[:k+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1ded4372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette(sample): 0.085\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>need_state_cluster</th>\n",
       "      <th>count</th>\n",
       "      <th>share_pct</th>\n",
       "      <th>basket_unique_items</th>\n",
       "      <th>qty</th>\n",
       "      <th>_revenue</th>\n",
       "      <th>order_hour</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>InFestival</th>\n",
       "      <th>weekday_boost</th>\n",
       "      <th>weekend_boost</th>\n",
       "      <th>festival_boost</th>\n",
       "      <th>hour_weight</th>\n",
       "      <th>loyalty_score</th>\n",
       "      <th>expected_basket_items</th>\n",
       "      <th>price_elasticity</th>\n",
       "      <th>top_components</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2530</td>\n",
       "      <td>13.19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.927</td>\n",
       "      <td>968.250</td>\n",
       "      <td>11.516</td>\n",
       "      <td>3.012</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.083</td>\n",
       "      <td>1.100</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.922</td>\n",
       "      <td>2.989</td>\n",
       "      <td>0.014</td>\n",
       "      <td>cat=Snacks:320.0; cat=Household:293.0; cat=Rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3509</td>\n",
       "      <td>18.30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.019</td>\n",
       "      <td>1065.008</td>\n",
       "      <td>11.574</td>\n",
       "      <td>3.061</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.080</td>\n",
       "      <td>1.001</td>\n",
       "      <td>1.049</td>\n",
       "      <td>1.050</td>\n",
       "      <td>1.004</td>\n",
       "      <td>0.924</td>\n",
       "      <td>2.989</td>\n",
       "      <td>0.007</td>\n",
       "      <td>cat=ReadyToEat:504.0; cat=Others:459.0; cat=Sn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>465</td>\n",
       "      <td>2.42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.133</td>\n",
       "      <td>1217.232</td>\n",
       "      <td>11.761</td>\n",
       "      <td>3.065</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.080</td>\n",
       "      <td>1.018</td>\n",
       "      <td>0.991</td>\n",
       "      <td>1.021</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.923</td>\n",
       "      <td>2.989</td>\n",
       "      <td>0.017</td>\n",
       "      <td>brand=Brand_023:462.0; cat=DairyBakery:113.0; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3151</td>\n",
       "      <td>16.43</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.979</td>\n",
       "      <td>1001.444</td>\n",
       "      <td>11.510</td>\n",
       "      <td>2.919</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.085</td>\n",
       "      <td>1.014</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.014</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.923</td>\n",
       "      <td>2.989</td>\n",
       "      <td>0.004</td>\n",
       "      <td>cat=ReadyToEat:422.0; cat=Snacks:420.0; cat=Ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1195</td>\n",
       "      <td>6.23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.906</td>\n",
       "      <td>663.389</td>\n",
       "      <td>11.552</td>\n",
       "      <td>2.862</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.074</td>\n",
       "      <td>1.016</td>\n",
       "      <td>1.003</td>\n",
       "      <td>1.024</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.925</td>\n",
       "      <td>2.990</td>\n",
       "      <td>0.007</td>\n",
       "      <td>cat=InstantFoods:1111.0; brand=Brand_036:176.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2472</td>\n",
       "      <td>12.89</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.028</td>\n",
       "      <td>1045.177</td>\n",
       "      <td>11.214</td>\n",
       "      <td>3.055</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.071</td>\n",
       "      <td>1.001</td>\n",
       "      <td>1.020</td>\n",
       "      <td>1.001</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.924</td>\n",
       "      <td>2.989</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>cat=ReadyToEat:336.0; cat=HealthBeauty:290.0; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2251</td>\n",
       "      <td>11.74</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.987</td>\n",
       "      <td>984.446</td>\n",
       "      <td>11.323</td>\n",
       "      <td>2.958</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.950</td>\n",
       "      <td>1.100</td>\n",
       "      <td>1.100</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.923</td>\n",
       "      <td>2.989</td>\n",
       "      <td>0.006</td>\n",
       "      <td>cat=ReadyToEat:265.0; cat=HealthBeauty:256.0; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>3605</td>\n",
       "      <td>18.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.987</td>\n",
       "      <td>1029.343</td>\n",
       "      <td>11.600</td>\n",
       "      <td>3.047</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.079</td>\n",
       "      <td>1.050</td>\n",
       "      <td>0.900</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.924</td>\n",
       "      <td>2.989</td>\n",
       "      <td>0.010</td>\n",
       "      <td>cat=Household:479.0; cat=ReadyToEat:447.0; cat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   need_state_cluster  count  share_pct  basket_unique_items    qty  _revenue  \\\n",
       "0                   0   2530      13.19                  1.0  2.927   968.250   \n",
       "1                   1   3509      18.30                  1.0  3.019  1065.008   \n",
       "2                   2    465       2.42                  1.0  3.133  1217.232   \n",
       "3                   3   3151      16.43                  1.0  2.979  1001.444   \n",
       "4                   4   1195       6.23                  1.0  2.906   663.389   \n",
       "5                   5   2472      12.89                  1.0  3.028  1045.177   \n",
       "6                   6   2251      11.74                  1.0  2.987   984.446   \n",
       "7                   7   3605      18.80                  1.0  2.987  1029.343   \n",
       "\n",
       "   order_hour  dayofweek  is_weekend  InFestival  weekday_boost  \\\n",
       "0      11.516      3.012       0.277       0.083          1.100   \n",
       "1      11.574      3.061       0.304       0.080          1.001   \n",
       "2      11.761      3.065       0.286       0.080          1.018   \n",
       "3      11.510      2.919       0.273       0.085          1.014   \n",
       "4      11.552      2.862       0.261       0.074          1.016   \n",
       "5      11.214      3.055       0.288       0.071          1.001   \n",
       "6      11.323      2.958       0.275       0.071          0.950   \n",
       "7      11.600      3.047       0.292       0.079          1.050   \n",
       "\n",
       "   weekend_boost  festival_boost  hour_weight  loyalty_score  \\\n",
       "0          0.879           0.950        0.999          0.922   \n",
       "1          1.049           1.050        1.004          0.924   \n",
       "2          0.991           1.021        0.994          0.923   \n",
       "3          1.000           1.014        0.996          0.923   \n",
       "4          1.003           1.024        0.992          0.925   \n",
       "5          1.020           1.001        0.993          0.924   \n",
       "6          1.100           1.100        0.997          0.923   \n",
       "7          0.900           1.000        0.995          0.924   \n",
       "\n",
       "   expected_basket_items  price_elasticity  \\\n",
       "0                  2.989             0.014   \n",
       "1                  2.989             0.007   \n",
       "2                  2.989             0.017   \n",
       "3                  2.989             0.004   \n",
       "4                  2.990             0.007   \n",
       "5                  2.989            -0.002   \n",
       "6                  2.989             0.006   \n",
       "7                  2.989             0.010   \n",
       "\n",
       "                                      top_components  \n",
       "0  cat=Snacks:320.0; cat=Household:293.0; cat=Rea...  \n",
       "1  cat=ReadyToEat:504.0; cat=Others:459.0; cat=Sn...  \n",
       "2  brand=Brand_023:462.0; cat=DairyBakery:113.0; ...  \n",
       "3  cat=ReadyToEat:422.0; cat=Snacks:420.0; cat=Ho...  \n",
       "4  cat=InstantFoods:1111.0; brand=Brand_036:176.0...  \n",
       "5  cat=ReadyToEat:336.0; cat=HealthBeauty:290.0; ...  \n",
       "6  cat=ReadyToEat:265.0; cat=HealthBeauty:256.0; ...  \n",
       "7  cat=Household:479.0; cat=ReadyToEat:447.0; cat...  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% Need-state discovery (fixed: auto-encode non-numeric) \n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# ทำ one-hot ให้ทุกคอลัมน์ที่เป็น object/category (กัน error 'Rainy')\n",
    "X_df = basket_feat[FEATURE_COLS].copy()\n",
    "\n",
    "# bool -> int\n",
    "bool_cols = X_df.select_dtypes(include=[\"bool\"]).columns\n",
    "if len(bool_cols):\n",
    "    X_df[bool_cols] = X_df[bool_cols].astype(int)\n",
    "\n",
    "obj_cols = X_df.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "if len(obj_cols):\n",
    "    X_df = pd.get_dummies(X_df, columns=obj_cols, dummy_na=True)\n",
    "\n",
    "X = X_df.fillna(0.0).astype(float).values\n",
    "\n",
    "# Scale + PCA\n",
    "sc = StandardScaler()\n",
    "Xs = sc.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=min(PCA_K, Xs.shape[1]), random_state=SEED)\n",
    "Xp  = pca.fit_transform(Xs)\n",
    "\n",
    "# KMeans\n",
    "mbk = MiniBatchKMeans(n_clusters=NEED_K, random_state=SEED, batch_size=4096, n_init=10)\n",
    "labels = mbk.fit_predict(Xp)\n",
    "basket_feat[\"need_state_cluster\"] = labels\n",
    "\n",
    "# silhouette (sample)\n",
    "try:\n",
    "    idx = np.random.RandomState(SEED).choice(len(Xp), size=min(5000, len(Xp)), replace=False)\n",
    "    sil = silhouette_score(Xp[idx], labels[idx])\n",
    "except Exception:\n",
    "    sil = np.nan\n",
    "print(f\"Silhouette(sample): {sil:.3f}\")\n",
    "\n",
    "# profiling\n",
    "prof_cols = [\n",
    "    \"basket_unique_items\", COL_QTY, COL_PRICE, \"_revenue\",\n",
    "    COL_ORDER_H, COL_DOW, COL_IS_WKD, COL_THAI_SEAS, COL_IN_FEST,\n",
    "    COL_WKD_BOOST, COL_WKE_BOOST, COL_FES_BOOST, COL_HOUR_W,\n",
    "    COL_LOYALTY, COL_EXPECT, COL_ELAS\n",
    "]\n",
    "prof_cols = [c for c in prof_cols if c in basket_feat.columns]\n",
    "\n",
    "def top_components(df_in, key, cols, n=8):\n",
    "    rows = []\n",
    "    for k, grp in df_in.groupby(key):\n",
    "        sums = grp[cols].sum().sort_values(ascending=False)\n",
    "        rows.append({key: k, \"top_components\": \"; \".join([f\"{c}:{sums[c]:.1f}\" for c in sums.index[:n]])})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "comp_cols = [c for c in basket_feat.columns if c.startswith(\"cat=\") or c.startswith(\"brand=\")]\n",
    "prof = (\n",
    "    basket_feat.groupby(\"need_state_cluster\")[prof_cols]\n",
    "    .mean(numeric_only=True).round(3).reset_index()\n",
    ")\n",
    "topc = top_components(basket_feat, \"need_state_cluster\", comp_cols, n=8) if comp_cols else pd.DataFrame(columns=[\"need_state_cluster\",\"top_components\"])\n",
    "\n",
    "need_profile = prof.merge(topc, on=\"need_state_cluster\", how=\"left\")\n",
    "need_profile.insert(1, \"count\", basket_feat.groupby(\"need_state_cluster\")[COL_TX].nunique().values)\n",
    "need_profile.insert(2, \"share_pct\", (need_profile[\"count\"]/need_profile[\"count\"].sum()*100).round(2))\n",
    "\n",
    "need_profile.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3b93423d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation report (P(type|X))\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Brandday       0.54      0.13      0.20       111\n",
      "   Buy 1 get 1       0.85      0.16      0.27       144\n",
      "    Flash Sale       0.00      0.00      0.00       303\n",
      "     Mega Sale       0.74      0.20      0.32       123\n",
      "       NoPromo       0.77      0.99      0.87      2904\n",
      "Product_Coupon       0.00      0.00      0.00       251\n",
      "\n",
      "      accuracy                           0.77      3836\n",
      "     macro avg       0.48      0.25      0.28      3836\n",
      "  weighted avg       0.65      0.77      0.68      3836\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# เตรียม label ต่อธุรกรรมจาก tx_merge โดยตรง (ถ้าไม่มี ใช้วิธี join ผ่าน promo_id แทน)\n",
    "if LABEL_COL_IN_TX not in tx_merge.columns:\n",
    "    raise ValueError(f\"ไม่พบ {LABEL_COL_IN_TX} ใน tx_merge\")\n",
    "\n",
    "label_df = (\n",
    "    tx_merge.groupby(COL_TX)[LABEL_COL_IN_TX].first().reset_index()\n",
    "    .rename(columns={LABEL_COL_IN_TX:\"used_type\"})\n",
    ")\n",
    "label_df[\"used_type\"] = label_df[\"used_type\"].fillna(\"NoPromo\")\n",
    "\n",
    "data_ptype = basket_feat.merge(label_df, on=COL_TX, how=\"left\")\n",
    "data_ptype[\"used_type\"] = data_ptype[\"used_type\"].fillna(\"NoPromo\")\n",
    "\n",
    "# one-hot ฟีเจอร์สำหรับทั้งชุด → คอลัมน์จะตรงกันแน่นอน\n",
    "X_all = data_ptype[FEATURE_COLS].copy()\n",
    "\n",
    "bool_cols = X_all.select_dtypes(include=[\"bool\"]).columns\n",
    "if len(bool_cols):\n",
    "    X_all[bool_cols] = X_all[bool_cols].astype(int)\n",
    "\n",
    "obj_cols = X_all.select_dtypes(include=[\"object\",\"category\"]).columns\n",
    "if len(obj_cols):\n",
    "    X_all = pd.get_dummies(X_all, columns=obj_cols, dummy_na=True)\n",
    "\n",
    "X_all = X_all.fillna(0.0).astype(float)\n",
    "\n",
    "# split ตามเวลา\n",
    "if \"event_time\" in data_ptype.columns and data_ptype[\"event_time\"].notna().any():\n",
    "    data_ptype = data_ptype.sort_values(\"event_time\")\n",
    "    X_all = X_all.loc[data_ptype.index]\n",
    "    cut = int(len(data_ptype)*0.8)\n",
    "    tr_idx = data_ptype.index[:cut]\n",
    "    va_idx = data_ptype.index[cut:]\n",
    "else:\n",
    "    tr_idx, va_idx = train_test_split(\n",
    "        data_ptype.index, test_size=0.2, random_state=SEED, stratify=data_ptype[\"used_type\"]\n",
    "    )\n",
    "\n",
    "Xtr = X_all.loc[tr_idx].values\n",
    "Xva = X_all.loc[va_idx].values\n",
    "ytr = data_ptype.loc[tr_idx, \"used_type\"].values\n",
    "yva = data_ptype.loc[va_idx, \"used_type\"].values\n",
    "\n",
    "classes = np.unique(data_ptype[\"used_type\"].values)\n",
    "class_to_idx = {c:i for i,c in enumerate(classes)}\n",
    "ytr_idx = np.array([class_to_idx[c] for c in ytr])\n",
    "yva_idx = np.array([class_to_idx[c] for c in yva])\n",
    "\n",
    "# base model + calibration (รองรับหลายเวอร์ชัน sklearn)\n",
    "if HAS_LGB:\n",
    "    base = lgb.LGBMClassifier(\n",
    "        objective=\"multiclass\",\n",
    "        num_class=len(classes),\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=63,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=SEED\n",
    "    )\n",
    "else:\n",
    "    base = GradientBoostingClassifier(random_state=SEED)\n",
    "\n",
    "try:\n",
    "    ptype_model = CalibratedClassifierCV(estimator=base, method=\"sigmoid\", cv=3)\n",
    "except TypeError:\n",
    "    ptype_model = CalibratedClassifierCV(base_estimator=base, method=\"sigmoid\", cv=3)\n",
    "\n",
    "ptype_model.fit(Xtr, ytr_idx)\n",
    "pred = ptype_model.predict(Xva)\n",
    "print(\"Validation report (P(type|X))\")\n",
    "print(classification_report(yva_idx, pred, target_names=list(classes)))\n",
    "\n",
    "ptype_classes  = list(classes)\n",
    "ptype_featcols = list(X_all.columns)  # สำคัญ: ใช้ตอน inference ต้อง align คอลัมน์ชุดนี้\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fa789576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_features_for_ptype(row_series, raw_feature_cols, feat_cols_all):\n",
    "    row_df = pd.DataFrame([row_series[raw_feature_cols]])\n",
    "    # bool -> int\n",
    "    bool_cols = row_df.select_dtypes(include=[\"bool\"]).columns\n",
    "    if len(bool_cols):\n",
    "        row_df[bool_cols] = row_df[bool_cols].astype(int)\n",
    "    # one-hot สำหรับ object/category\n",
    "    obj_cols = row_df.select_dtypes(include=[\"object\",\"category\"]).columns\n",
    "    if len(obj_cols):\n",
    "        row_df = pd.get_dummies(row_df, columns=obj_cols, dummy_na=True)\n",
    "    # align columns\n",
    "    for c in feat_cols_all:\n",
    "        if c not in row_df.columns:\n",
    "            row_df[c] = 0.0\n",
    "    row_df = row_df[feat_cols_all].fillna(0.0).astype(float)\n",
    "    return row_df.values  # shape (1, d)\n",
    "\n",
    "def eligibility_filter(promos_df, context_row, now):\n",
    "    out = promos_df.copy()\n",
    "    if \"start_date\" in out.columns:\n",
    "        out[\"start_date\"] = pd.to_datetime(out[\"start_date\"], errors=\"coerce\")\n",
    "    if \"end_date\" in out.columns:\n",
    "        out[\"end_date\"] = pd.to_datetime(out[\"end_date\"], errors=\"coerce\")\n",
    "    if \"is_online\" in out.columns and COL_ONLINE in context_row.index:\n",
    "        out = out[out[\"is_online\"] == int(context_row[COL_ONLINE])]\n",
    "    if \"start_date\" in out.columns and \"end_date\" in out.columns and pd.notna(now):\n",
    "        out = out[(out[\"start_date\"] <= now) & (now <= out[\"end_date\"])]\n",
    "    return out\n",
    "\n",
    "# แทนที่ฟังก์ชันเดิมทั้งก้อน\n",
    "def simple_scope_relevance(basket_row, promo_row):\n",
    "    \"\"\"\n",
    "    คำนวณความเกี่ยวข้องระหว่างโปรกับตะกร้า\n",
    "    - ถ้า product_scope มี category/code: วัด Jaccard กับ cat=... ในบิล\n",
    "    - ถ้า scope ว่าง: ลดน้ำหนักลง ตามความนิยมของหมวดในบิล (ไม่ใช่ 0.5 ตายตัว)\n",
    "    \"\"\"\n",
    "    scope_raw = str(promo_row.get(\"product_scope\", \"\") or \"\").strip().lower()\n",
    "    # ดึงหมวดในบิล (จากฟีเจอร์ cat=... ที่เป็นสัดส่วน)\n",
    "    basket_cats = {col.split(\"cat=\")[1].lower() for col in basket_row.index\n",
    "                   if isinstance(col, str) and col.startswith(\"cat=\") and float(basket_row[col]) > 0}\n",
    "\n",
    "    if not basket_cats:\n",
    "        return 0.15  # ไม่มีสัดส่วนหมวด → ให้ต่ำหน่อย\n",
    "\n",
    "    # เคสมี scope → tokenize เป็นชุดคำ (รองรับ comma, ;, space)\n",
    "    if scope_raw:\n",
    "        sep = [\",\",\";\",\"|\",\"/\"]\n",
    "        for s in sep: scope_raw = scope_raw.replace(s, \" \")\n",
    "        scope_set = {tok for tok in scope_raw.split() if tok}\n",
    "        if not scope_set:\n",
    "            return 0.2\n",
    "        inter = len(basket_cats & scope_set)\n",
    "        union = len(basket_cats | scope_set)\n",
    "        j = inter/union if union else 0.0\n",
    "        # เพิ่ม boost ถ้า inter>0\n",
    "        bonus = 0.2 if inter > 0 else 0.0\n",
    "        return min(1.0, 0.3 + 0.7*j + bonus)\n",
    "\n",
    "    # เคส scope ว่าง → ให้คะแนนตามความ “กระจุกตัว” ของหมวดในบิล\n",
    "    # ยิ่งบิลมี 1-2 หมวดหลักชัดเจน → relevance สูงขึ้น (โปรจับหมวดกว้างก็ยังพอเวิร์ก)\n",
    "    cat_share = [float(basket_row[c]) for c in basket_row.index\n",
    "                 if isinstance(c, str) and c.startswith(\"cat=\")]\n",
    "    if not cat_share:\n",
    "        return 0.2\n",
    "    top_share = sorted(cat_share, reverse=True)[:2]\n",
    "    focus = sum(top_share)  # ~ 0.6–1.0 ถ้าบิลโฟกัสหมวดชัด\n",
    "    return max(0.2, min(0.7, 0.3 + 0.4*focus))\n",
    "\n",
    "\n",
    "def recall_candidates_for_event_relaxed(\n",
    "    basket_row,\n",
    "    promos_df,\n",
    "    probs, classes,\n",
    "    topk_types=2,\n",
    "    relevance_thresh=0.30,\n",
    "    nopromo_label=\"NoPromo\"\n",
    "):\n",
    "    # 2.1 เลือกประเภท robust\n",
    "    top_types = get_top_types(probs, classes, k=topk_types, ensure_non_nopromo=2, nopromo_label=nopromo_label)\n",
    "    now = basket_row.get(\"event_time\", pd.NaT)\n",
    "\n",
    "    def _elig(df, strict_online=True):\n",
    "        out = df.copy()\n",
    "        if \"start_date\" in out.columns and \"end_date\" in out.columns and pd.notna(now):\n",
    "            out = out[(out[\"start_date\"] <= now) & (now <= out[\"end_date\"])]\n",
    "        if strict_online and \"is_online\" in out.columns and \"is_online\" in basket_row.index:\n",
    "            out = out[out[\"is_online\"] == int(basket_row[\"is_online\"])]\n",
    "        return out\n",
    "\n",
    "    def _score_scope(df_):\n",
    "        df_ = df_.copy()\n",
    "        df_[\"scope_relevance\"] = df_.apply(lambda r: simple_scope_relevance(basket_row, r), axis=1)\n",
    "        return df_\n",
    "\n",
    "    # Stage 1: เข้มที่สุด — date+channel + type filter\n",
    "    cand = _elig(promos_df, strict_online=True)\n",
    "    if \"promo_type\" in cand.columns:\n",
    "        cand = cand[cand[\"promo_type\"].isin(top_types)]\n",
    "    cand = _score_scope(cand)\n",
    "    out = cand[cand[\"scope_relevance\"] >= relevance_thresh]\n",
    "\n",
    "    # Stage 2: ผ่อน channel (online/offline)\n",
    "    if out.empty:\n",
    "        cand2 = _elig(promos_df, strict_online=False)\n",
    "        if \"promo_type\" in cand2.columns:\n",
    "            cand2 = cand2[cand2[\"promo_type\"].isin(top_types)]\n",
    "        cand2 = _score_scope(cand2)\n",
    "        out = cand2[cand2[\"scope_relevance\"] >= max(0.2, relevance_thresh*0.75)]\n",
    "\n",
    "    # Stage 3: ผ่อน type filter (เลือกตาม scope สูงสุดแทน)\n",
    "    if out.empty:\n",
    "        cand3 = _elig(promos_df, strict_online=False)\n",
    "        cand3 = _score_scope(cand3)\n",
    "        out = cand3.nlargest(20, \"scope_relevance\")  # ดึงมาบางส่วนให้มีตัวเลือก\n",
    "\n",
    "    # เติม NoPromo ไว้เป็น baseline เสมอ\n",
    "    nopromo = pd.DataFrame([{\n",
    "        \"promo_id\": \"__NOPROMO__\", \"promo_type\": nopromo_label,\n",
    "        \"product_scope\": \"\", \"est_margin\": 0.0, \"scope_relevance\": 0.0\n",
    "    }])\n",
    "    return pd.concat([out, nopromo], ignore_index=True).drop_duplicates(subset=[\"promo_id\"], keep=\"first\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ba9e509d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>promo_id</th>\n",
       "      <th>promo_type</th>\n",
       "      <th>ptype_prob</th>\n",
       "      <th>scope_relevance</th>\n",
       "      <th>est_margin</th>\n",
       "      <th>is_online</th>\n",
       "      <th>order_hour</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>need_state_cluster</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PMTX0000001</td>\n",
       "      <td>PR0005</td>\n",
       "      <td>Buy 1 get 1</td>\n",
       "      <td>0.519983</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PMTX0000001</td>\n",
       "      <td>PR0021</td>\n",
       "      <td>Buy 1 get 1</td>\n",
       "      <td>0.519983</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PMTX0000001</td>\n",
       "      <td>PR0030</td>\n",
       "      <td>Buy 1 get 1</td>\n",
       "      <td>0.519983</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PMTX0000001</td>\n",
       "      <td>PR0034</td>\n",
       "      <td>Buy 1 get 1</td>\n",
       "      <td>0.519983</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PMTX0000001</td>\n",
       "      <td>PR0048</td>\n",
       "      <td>Buy 1 get 1</td>\n",
       "      <td>0.519983</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      event_id promo_id   promo_type  ptype_prob  scope_relevance  est_margin  \\\n",
       "0  PMTX0000001   PR0005  Buy 1 get 1    0.519983              0.7         0.0   \n",
       "1  PMTX0000001   PR0021  Buy 1 get 1    0.519983              0.7         0.0   \n",
       "2  PMTX0000001   PR0030  Buy 1 get 1    0.519983              0.7         0.0   \n",
       "3  PMTX0000001   PR0034  Buy 1 get 1    0.519983              0.7         0.0   \n",
       "4  PMTX0000001   PR0048  Buy 1 get 1    0.519983              0.7         0.0   \n",
       "\n",
       "   is_online  order_hour  dayofweek  need_state_cluster  label  \n",
       "0          0           9          0                   0      1  \n",
       "1          0           9          0                   0      1  \n",
       "2          0           9          0                   0      1  \n",
       "3          0           9          0                   0      1  \n",
       "4          0           9          0                   0      1  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_ranking_frame(basket_feats, ptype_model, ptype_classes, ptype_featcols,\n",
    "                        promos_df, label_df, topk=TOPK_TYPES, max_cands=MAX_CANDS):\n",
    "    class_to_idx = {c:i for i,c in enumerate(ptype_classes)}\n",
    "    data = basket_feats.merge(label_df, on=COL_TX, how=\"left\")\n",
    "    data[\"used_type\"] = data[\"used_type\"].fillna(\"NoPromo\")\n",
    "\n",
    "    rows = []\n",
    "    for _, row in data.iterrows():\n",
    "        # encode ให้คอลัมน์ one-hot ตรงกับตอนเทรน\n",
    "        X = encode_features_for_ptype(row, FEATURE_COLS, ptype_featcols)\n",
    "        probs = ptype_model.predict_proba(X)[0]\n",
    "\n",
    "        cands = recall_candidates_for_event_relaxed(\n",
    "            basket_row=row,\n",
    "            promos_df=promos_df,\n",
    "            probs=probs,\n",
    "            classes=ptype_classes,\n",
    "            topk_types=TOPK_TYPES,\n",
    "            relevance_thresh=REL_TH,\n",
    "            nopromo_label=\"NoPromo\"\n",
    "        )\n",
    "\n",
    "        if len(cands) > max_cands:\n",
    "            cands = pd.concat([\n",
    "                cands.nlargest(max_cands//2, \"scope_relevance\"),\n",
    "                cands.sample(n=max_cands-(max_cands//2), random_state=SEED, replace=False)\n",
    "            ])\n",
    "\n",
    "        used_type = row[\"used_type\"]\n",
    "        for _, pr in cands.iterrows():\n",
    "            label = 1 if (pr[\"promo_type\"] == used_type or (used_type==\"NoPromo\" and pr[\"promo_id\"]==\"__NOPROMO__\")) else 0\n",
    "            rows.append({\n",
    "                \"event_id\": row[COL_TX],\n",
    "                \"promo_id\": pr[\"promo_id\"],\n",
    "                \"promo_type\": pr[\"promo_type\"],\n",
    "                \"ptype_prob\": float(probs[class_to_idx.get(pr[\"promo_type\"], class_to_idx.get(\"NoPromo\", 0))]),\n",
    "                \"scope_relevance\": pr.get(\"scope_relevance\", 0.0),\n",
    "                \"est_margin\": pr.get(\"est_margin\", 0.0),\n",
    "                \"is_online\": row.get(COL_ONLINE, 0),\n",
    "                \"order_hour\": row.get(COL_ORDER_H, 0),\n",
    "                \"dayofweek\": row.get(COL_DOW, 0),\n",
    "                \"need_state_cluster\": row.get(\"need_state_cluster\", 0),\n",
    "                \"label\": label\n",
    "            })\n",
    "    rank_df = pd.DataFrame(rows)\n",
    "\n",
    "    # cap negatives per event\n",
    "    out = []\n",
    "    for eid, grp in rank_df.groupby(\"event_id\"):\n",
    "        pos = grp[grp[\"label\"]==1]\n",
    "        neg = grp[grp[\"label\"]==0]\n",
    "        keep_neg = neg if len(neg) <= (max_cands - len(pos)) else neg.sample(n=max_cands - len(pos), random_state=SEED)\n",
    "        out.append(pd.concat([pos, keep_neg], ignore_index=True))\n",
    "    return pd.concat(out, ignore_index=True)\n",
    "\n",
    "rank_df = build_ranking_frame(\n",
    "    basket_feats=basket_feat,\n",
    "    ptype_model=ptype_model,\n",
    "    ptype_classes=ptype_classes,\n",
    "    ptype_featcols=ptype_featcols,\n",
    "    promos_df=promos_df,\n",
    "    label_df=label_df,\n",
    "    topk=TOPK_TYPES,\n",
    "    max_cands=MAX_CANDS\n",
    ")\n",
    "rank_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c70ffefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# หลังสร้าง rank_df = pd.DataFrame(rows)\n",
    "# bring event_time\n",
    "rank_df = rank_df.merge(\n",
    "    basket_feat[[COL_TX, \"event_time\"]].drop_duplicates(),\n",
    "    left_on=\"event_id\", right_on=COL_TX, how=\"left\"\n",
    ").drop(columns=[COL_TX])\n",
    "\n",
    "# parse dates\n",
    "for c in [\"start_date\",\"end_date\"]:\n",
    "    if c in rank_df.columns:\n",
    "        rank_df[c] = pd.to_datetime(rank_df[c], errors=\"coerce\")\n",
    "\n",
    "# new features (เหมือน patch ด้านบน)\n",
    "rank_df[\"discount_norm\"] = (rank_df[\"discount\"].astype(float).fillna(0) / 100.0) if \"discount\" in rank_df.columns else 0.0\n",
    "\n",
    "rank_df[\"is_active_now\"] = (\n",
    "    (rank_df[\"start_date\"] <= rank_df[\"event_time\"]) &\n",
    "    (rank_df[\"event_time\"] <= rank_df[\"end_date\"])\n",
    ").astype(int) if {\"start_date\",\"end_date\",\"event_time\"}.issubset(rank_df.columns) else 1\n",
    "\n",
    "rank_df[\"days_to_end\"] = (\n",
    "    (rank_df[\"end_date\"] - rank_df[\"event_time\"]).dt.days.fillna(0).clip(lower=-365, upper=365)\n",
    ") if {\"end_date\",\"event_time\"}.issubset(rank_df.columns) else 0\n",
    "\n",
    "rank_df[\"type_dup_penalty\"] = (\n",
    "    rank_df.groupby([\"event_id\",\"promo_type\"])[\"promo_id\"].transform(\"count\") - 1\n",
    ").clip(lower=0).fillna(0)\n",
    "\n",
    "rank_df[\"dup_product_penalty\"] = (\n",
    "    rank_df.groupby([\"event_id\",\"product_id\"])[\"promo_id\"].transform(\"count\") - 1\n",
    ").clip(lower=0).fillna(0) if \"product_id\" in rank_df.columns else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "78311745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttrain's ndcg@3: 0.992347\ttrain's ndcg@5: 0.994588\tvalid's ndcg@3: 0.98875\tvalid's ndcg@5: 0.990832\n",
      "Early stopping, best iteration is:\n",
      "[70]\ttrain's ndcg@3: 0.990703\ttrain's ndcg@5: 0.993287\tvalid's ndcg@3: 0.989239\tvalid's ndcg@5: 0.99116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ndcg@3': 0.9716150422727791, 'ndcg@5': 0.9728065864606266}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ndcg_at_k(rels, k=5):\n",
    "    rels = np.asfarray(rels)[:k]\n",
    "    if rels.size == 0: return 0.0\n",
    "    dcg = np.sum((2**rels - 1) / np.log2(np.arange(2, rels.size + 2)))\n",
    "    ideal = np.sort(rels)[::-1]\n",
    "    idcg = np.sum((2**ideal - 1) / np.log2(np.arange(2, ideal.size + 2)))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def train_ranker(rank_df, k_list=(3,5)):\n",
    "    F = [\"ptype_prob\",\"scope_relevance\",\"est_margin\",\n",
    "     \"discount_norm\",\"is_active_now\",\"days_to_end\",\n",
    "     \"type_dup_penalty\",\"dup_product_penalty\",\n",
    "     \"is_online\",\"order_hour\",\"dayofweek\",\"need_state_cluster\"]\n",
    "\n",
    "    ev = rank_df[\"event_id\"].unique()\n",
    "    tr_e, va_e = train_test_split(ev, test_size=0.2, random_state=SEED)\n",
    "    tr = rank_df[rank_df[\"event_id\"].isin(tr_e)]\n",
    "    va = rank_df[rank_df[\"event_id\"].isin(va_e)]\n",
    "\n",
    "    def to_group(df_):\n",
    "        grp_sizes = df_.groupby(\"event_id\").size().values\n",
    "        X = df_[F].fillna(0).values\n",
    "        y = df_[\"label\"].values\n",
    "        return X, y, grp_sizes\n",
    "\n",
    "    if HAS_LGB:\n",
    "        Xtr, ytr, gtr = to_group(tr)\n",
    "        Xva, yva, gva = to_group(va)\n",
    "\n",
    "        # ----- core API with callbacks (รองรับหลายเวอร์ชัน) -----\n",
    "        try:\n",
    "            dtr = lgb.Dataset(Xtr, label=ytr, group=gtr)\n",
    "            dva = lgb.Dataset(Xva, label=yva, group=gva, reference=dtr)\n",
    "            params = dict(\n",
    "                objective=\"lambdarank\",\n",
    "                metric=\"ndcg\",          # <--- สำคัญ: ใช้ 'ndcg' + eval_at แทน 'ndcg@k'\n",
    "                eval_at=[3, 5],        # <--- ระบุ k ที่ต้องการประเมิน\n",
    "                learning_rate=0.05,\n",
    "                num_leaves=63,\n",
    "                min_data_in_leaf=100,\n",
    "                feature_fraction=0.8,\n",
    "                bagging_fraction=0.8,\n",
    "                bagging_freq=1,\n",
    "                verbosity=-1,\n",
    "                seed=SEED\n",
    "            )\n",
    "            cbs = []\n",
    "            # ใส่ early_stopping ผ่าน callback (บางเวอร์ชันเท่านั้น)\n",
    "            try:\n",
    "                cbs.append(lgb.early_stopping(stopping_rounds=100))\n",
    "            except Exception:\n",
    "                pass\n",
    "            # ใส่ log interval ถ้ามี\n",
    "            try:\n",
    "                cbs.append(lgb.log_evaluation(100))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                model = lgb.train(\n",
    "                    params,\n",
    "                    dtr,\n",
    "                    num_boost_round=800,\n",
    "                    valid_sets=[dtr, dva],\n",
    "                    valid_names=[\"train\",\"valid\"],\n",
    "                    callbacks=cbs\n",
    "                )\n",
    "            except ValueError:\n",
    "                # ถ้ายัง complain เรื่อง metric/early stopping ให้รันแบบไม่มี early stopping\n",
    "                model = lgb.train(\n",
    "                    params,\n",
    "                    dtr,\n",
    "                    num_boost_round=800,\n",
    "                    valid_sets=[dtr, dva],\n",
    "                    valid_names=[\"train\",\"valid\"]\n",
    "                )\n",
    "            use_core_api = True\n",
    "\n",
    "        except Exception:\n",
    "            # ----- fallback เป็น sklearn API LGBMRanker -----\n",
    "            ranker = lgb.LGBMRanker(\n",
    "                objective=\"lambdarank\",\n",
    "                n_estimators=800,\n",
    "                learning_rate=0.05,\n",
    "                num_leaves=63,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=SEED\n",
    "            )\n",
    "            try:\n",
    "                # บางเวอร์ชันรองรับ eval_at ผ่าน set_params\n",
    "                ranker.set_params(metric=\"ndcg\", eval_at=[3,5])\n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                ranker.fit(\n",
    "                    Xtr, ytr,\n",
    "                    group=gtr.tolist(),\n",
    "                    eval_set=[(Xva, yva)],\n",
    "                    eval_group=[gva.tolist()]\n",
    "                )\n",
    "            except TypeError:\n",
    "                ranker.fit(Xtr, ytr, group=gtr.tolist())\n",
    "            model = ranker\n",
    "            use_core_api = False\n",
    "\n",
    "        # ----- ประเมิน NDCG -----\n",
    "        ndcgs = {f\"ndcg@{k}\":[] for k in k_list}\n",
    "        for eid, grp in va.groupby(\"event_id\"):\n",
    "            if use_core_api:\n",
    "                s = model.predict(grp[F].fillna(0).values,\n",
    "                                  num_iteration=getattr(model, \"best_iteration\", None))\n",
    "            else:\n",
    "                s = model.predict(grp[F].fillna(0).values)\n",
    "            grp = grp.assign(_s=s).sort_values(\"_s\", ascending=False)\n",
    "            for k in k_list:\n",
    "                ndcgs[f\"ndcg@{k}\"].append(ndcg_at_k(grp[\"label\"].values, k))\n",
    "        return {\"model\": model, \"feature_cols\": F, \"report\": {m: float(np.mean(v)) for m,v in ndcgs.items()}}\n",
    "\n",
    "    else:\n",
    "        # Fallback: pointwise classifier\n",
    "        clf = GradientBoostingClassifier(random_state=SEED)\n",
    "        Xtr, ytr, _ = to_group(tr)\n",
    "        Xva, yva, _ = to_group(va)\n",
    "        clf.fit(Xtr, ytr)\n",
    "        ndcgs = {f\"ndcg@{k}\":[] for k in k_list}\n",
    "        for eid, grp in va.groupby(\"event_id\"):\n",
    "            s = clf.predict_proba(grp[F].fillna(0).values)[:,1]\n",
    "            grp = grp.assign(_s=s).sort_values(\"_s\", ascending=False)\n",
    "            for k in k_list:\n",
    "                ndcgs[f\"ndcg@{k}\"].append(ndcg_at_k(grp[\"label\"].values, k))\n",
    "        return {\"model\": clf, \"feature_cols\": F, \"report\": {m: float(np.mean(v)) for m,v in ndcgs.items()}, \"fallback_pointwise\": True}\n",
    "\n",
    "\n",
    "\n",
    "rank_art = train_ranker(rank_df)\n",
    "rank_art[\"report\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cac66307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>promo_id</th>\n",
       "      <th>promo_type</th>\n",
       "      <th>product_id</th>\n",
       "      <th>discount</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>product_scope</th>\n",
       "      <th>is_online</th>\n",
       "      <th>est_margin</th>\n",
       "      <th>scope_relevance</th>\n",
       "      <th>...</th>\n",
       "      <th>order_hour</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>need_state_cluster</th>\n",
       "      <th>is_active_now</th>\n",
       "      <th>days_to_end</th>\n",
       "      <th>discount_norm</th>\n",
       "      <th>type_dup_penalty</th>\n",
       "      <th>dup_product_penalty</th>\n",
       "      <th>ranker_score</th>\n",
       "      <th>final_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__NOPROMO__</td>\n",
       "      <td>NoPromo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.578439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PR0026</td>\n",
       "      <td>Mega Sale</td>\n",
       "      <td>P0527</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2025-08-14 01:45:00</td>\n",
       "      <td>2025-09-18 01:45:00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.29</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PR0077</td>\n",
       "      <td>Mega Sale</td>\n",
       "      <td>P0404</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2025-07-26 01:24:00</td>\n",
       "      <td>2025-09-15 01:24:00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.29</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PR0006</td>\n",
       "      <td>Mega Sale</td>\n",
       "      <td>P0107</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2025-08-01 16:08:00</td>\n",
       "      <td>2025-09-21 16:08:00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PR0090</td>\n",
       "      <td>Mega Sale</td>\n",
       "      <td>P0763</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2025-07-23 03:25:00</td>\n",
       "      <td>2025-09-20 03:25:00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PR0088</td>\n",
       "      <td>Product_Coupon</td>\n",
       "      <td>P0623</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2025-09-04 09:56:00</td>\n",
       "      <td>2025-09-13 09:56:00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030086</td>\n",
       "      <td>-0.173355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PR0069</td>\n",
       "      <td>Product_Coupon</td>\n",
       "      <td>P0925</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2025-09-07 08:34:00</td>\n",
       "      <td>2025-09-20 08:34:00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.43</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030086</td>\n",
       "      <td>-0.177921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PR0067</td>\n",
       "      <td>Product_Coupon</td>\n",
       "      <td>P0182</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2025-08-11 02:29:00</td>\n",
       "      <td>2025-09-21 02:29:00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.29</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030086</td>\n",
       "      <td>-0.190253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PR0003</td>\n",
       "      <td>Product_Coupon</td>\n",
       "      <td>P0441</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2025-08-04 01:41:00</td>\n",
       "      <td>2025-09-11 01:41:00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030086</td>\n",
       "      <td>-0.195151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PR0068</td>\n",
       "      <td>Product_Coupon</td>\n",
       "      <td>P0183</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2025-07-24 13:49:00</td>\n",
       "      <td>2025-09-20 13:49:00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030086</td>\n",
       "      <td>-0.195151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      promo_id      promo_type product_id  discount          start_date  \\\n",
       "0  __NOPROMO__         NoPromo        NaN       NaN                 NaT   \n",
       "1       PR0026       Mega Sale      P0527      29.0 2025-08-14 01:45:00   \n",
       "2       PR0077       Mega Sale      P0404      29.0 2025-07-26 01:24:00   \n",
       "3       PR0006       Mega Sale      P0107      22.0 2025-08-01 16:08:00   \n",
       "4       PR0090       Mega Sale      P0763      14.0 2025-07-23 03:25:00   \n",
       "5       PR0088  Product_Coupon      P0623      48.0 2025-09-04 09:56:00   \n",
       "6       PR0069  Product_Coupon      P0925      43.0 2025-09-07 08:34:00   \n",
       "7       PR0067  Product_Coupon      P0182      29.0 2025-08-11 02:29:00   \n",
       "8       PR0003  Product_Coupon      P0441      25.0 2025-08-04 01:41:00   \n",
       "9       PR0068  Product_Coupon      P0183      25.0 2025-07-24 13:49:00   \n",
       "\n",
       "             end_date product_scope  is_online  est_margin  scope_relevance  \\\n",
       "0                 NaT                        0         0.0              0.0   \n",
       "1 2025-09-18 01:45:00                        0         0.0              0.7   \n",
       "2 2025-09-15 01:24:00                        0         0.0              0.7   \n",
       "3 2025-09-21 16:08:00                        0         0.0              0.7   \n",
       "4 2025-09-20 03:25:00                        0         0.0              0.7   \n",
       "5 2025-09-13 09:56:00                        0         0.0              0.7   \n",
       "6 2025-09-20 08:34:00                        0         0.0              0.7   \n",
       "7 2025-09-21 02:29:00                        0         0.0              0.7   \n",
       "8 2025-09-11 01:41:00                        0         0.0              0.7   \n",
       "9 2025-09-20 13:49:00                        0         0.0              0.7   \n",
       "\n",
       "   ...  order_hour  dayofweek  need_state_cluster  is_active_now  days_to_end  \\\n",
       "0  ...          17          1                   3              0          0.0   \n",
       "1  ...          17          1                   3              1          8.0   \n",
       "2  ...          17          1                   3              1          5.0   \n",
       "3  ...          17          1                   3              1         11.0   \n",
       "4  ...          17          1                   3              1         10.0   \n",
       "5  ...          17          1                   3              1          3.0   \n",
       "6  ...          17          1                   3              1         10.0   \n",
       "7  ...          17          1                   3              1         11.0   \n",
       "8  ...          17          1                   3              1          1.0   \n",
       "9  ...          17          1                   3              1         10.0   \n",
       "\n",
       "   discount_norm  type_dup_penalty  dup_product_penalty  ranker_score  \\\n",
       "0           0.00                 0                  0.0      1.000000   \n",
       "1           0.29                 3                  0.0      0.000000   \n",
       "2           0.29                 3                  0.0      0.000000   \n",
       "3           0.22                 3                  0.0      0.000000   \n",
       "4           0.14                 3                  0.0      0.000000   \n",
       "5           0.48                 8                  0.0      0.030086   \n",
       "6           0.43                 8                  0.0      0.030086   \n",
       "7           0.29                 8                  0.0      0.030086   \n",
       "8           0.25                 8                  0.0      0.030086   \n",
       "9           0.25                 8                  0.0      0.030086   \n",
       "\n",
       "   final_score  \n",
       "0     0.578439  \n",
       "1     0.049096  \n",
       "2     0.049096  \n",
       "3     0.040666  \n",
       "4     0.032567  \n",
       "5    -0.173355  \n",
       "6    -0.177921  \n",
       "7    -0.190253  \n",
       "8    -0.195151  \n",
       "9    -0.195151  \n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def score_event(event_tx_id, basket_feats, ptype_model, ptype_classes, ptype_featcols,\n",
    "                promos_df, rank_art, topk=TOPK_TYPES, rel_th=REL_TH):\n",
    "    # 0) ดึงแถวบริบท\n",
    "    row = basket_feats[basket_feats[COL_TX]==event_tx_id]\n",
    "    if row.empty:\n",
    "        raise ValueError(\"transaction_id ไม่พบใน basket_feats\")\n",
    "    row = row.iloc[0]\n",
    "\n",
    "    # 1) prior P(type|X)\n",
    "    X = encode_features_for_ptype(row, FEATURE_COLS, ptype_featcols)\n",
    "    probs = ptype_model.predict_proba(X)[0]\n",
    "    class_to_idx = {c:i for i,c in enumerate(ptype_classes)}\n",
    "\n",
    "    # 2) recall (แบบ relaxed)\n",
    "    cands = recall_candidates_for_event_relaxed(\n",
    "        basket_row=row,\n",
    "        promos_df=promos_df,\n",
    "        probs=probs,\n",
    "        classes=ptype_classes,\n",
    "        topk_types=TOPK_TYPES,\n",
    "        relevance_thresh=rel_th,\n",
    "        nopromo_label=\"NoPromo\"\n",
    "    )\n",
    "\n",
    "    # 3) เตรียมฟีเจอร์ให้ครบสำหรับ ranker (เติม \"ก่อน\" ใช้ F)\n",
    "    tmp = cands.copy()\n",
    "\n",
    "    # prior prob ต่อโปรชนิดนั้น\n",
    "    tmp[\"ptype_prob\"] = tmp[\"promo_type\"].apply(\n",
    "        lambda t: probs[class_to_idx.get(t, class_to_idx.get(\"NoPromo\", 0))]\n",
    "    )\n",
    "\n",
    "    # บริบทเหตุการณ์\n",
    "    tmp[\"is_online\"] = int(row.get(COL_ONLINE, 0))\n",
    "    tmp[\"order_hour\"] = int(row.get(COL_ORDER_H, 0))\n",
    "    tmp[\"dayofweek\"] = int(row.get(COL_DOW, 0))\n",
    "    tmp[\"need_state_cluster\"] = int(row.get(\"need_state_cluster\", 0))\n",
    "\n",
    "    # วันที่/ช่วงโปร\n",
    "    now = row.get(\"event_time\", pd.NaT)\n",
    "    if \"start_date\" in tmp.columns and \"end_date\" in tmp.columns and pd.notna(now):\n",
    "        tmp[\"is_active_now\"] = ((tmp[\"start_date\"] <= now) & (now <= tmp[\"end_date\"])).astype(int)\n",
    "        tmp[\"days_to_end\"] = (tmp[\"end_date\"] - now).dt.days.clip(lower=-365, upper=365)\n",
    "    else:\n",
    "        tmp[\"is_active_now\"] = 1\n",
    "        tmp[\"days_to_end\"] = 0\n",
    "\n",
    "    # ส่วนลด normalize\n",
    "    if \"discount\" in tmp.columns:\n",
    "        tmp[\"discount_norm\"] = pd.to_numeric(tmp[\"discount\"], errors=\"coerce\").fillna(0) / 100.0\n",
    "    else:\n",
    "        tmp[\"discount_norm\"] = 0.0\n",
    "\n",
    "    # penalties ในกลุ่มเดียวกัน\n",
    "    tmp[\"type_dup_penalty\"] = (\n",
    "        tmp.groupby(\"promo_type\")[\"promo_id\"].transform(\"count\") - 1\n",
    "    ).clip(lower=0).fillna(0)\n",
    "\n",
    "    if \"product_id\" in tmp.columns:\n",
    "        tmp[\"dup_product_penalty\"] = (\n",
    "            tmp.groupby(\"product_id\")[\"promo_id\"].transform(\"count\") - 1\n",
    "        ).clip(lower=0).fillna(0)\n",
    "    else:\n",
    "        tmp[\"dup_product_penalty\"] = 0.0\n",
    "\n",
    "    # กัน missing ที่ ranker ต้องใช้\n",
    "    needed = [\"ptype_prob\",\"scope_relevance\",\"est_margin\",\n",
    "              \"discount_norm\",\"is_active_now\",\"days_to_end\",\n",
    "              \"type_dup_penalty\",\"dup_product_penalty\",\n",
    "              \"is_online\",\"order_hour\",\"dayofweek\",\"need_state_cluster\"]\n",
    "    for c in needed:\n",
    "        if c not in tmp.columns:\n",
    "            tmp[c] = 0.0\n",
    "    tmp[needed] = tmp[needed].fillna(0)\n",
    "\n",
    "    # 4) จัดอันดับด้วย ranker\n",
    "    F = rank_art[\"feature_cols\"]  # ต้องตรงกับตอนเทรน\n",
    "    mdl = rank_art[\"model\"]\n",
    "    Xr = tmp[F].fillna(0).values\n",
    "\n",
    "    if HAS_LGB and \"fallback_pointwise\" not in rank_art:\n",
    "        s = mdl.predict(Xr, num_iteration=getattr(mdl, \"best_iteration\", None))\n",
    "    else:\n",
    "        s = mdl.predict_proba(Xr)[:, 1]\n",
    "\n",
    "    # normalize และ tie-breaker\n",
    "    s_ptp = float(np.ptp(s))\n",
    "    tmp[\"ranker_score\"] = (s - float(np.min(s))) / s_ptp if s_ptp > 1e-9 else s\n",
    "    if tmp[\"ranker_score\"].nunique() == 1:\n",
    "        tb = (tmp[\"promo_id\"].astype(str).apply(lambda x: (hash(x) % 997) / 997.0)) * 0.01\n",
    "        tmp[\"ranker_score\"] = tmp[\"ranker_score\"] + tb\n",
    "\n",
    "    # 5) blend คะแนนสุดท้าย (หลังมีทุกฟีเจอร์แล้ว)\n",
    "    w = {\n",
    "        \"ptype_prob\": 0.28,\n",
    "        \"ranker_score\": 0.38,\n",
    "        \"scope_relevance\": 0.15,\n",
    "        \"est_margin\": 0.06,\n",
    "        \"discount_norm\": 0.08,\n",
    "        \"is_active_now\": 0.05\n",
    "    }\n",
    "    pen = {\"type_dup_penalty\": 0.05, \"dup_product_penalty\": 0.08}\n",
    "\n",
    "    # tie-break helper: combine monotonic positives to reduce equal scores\n",
    "    tie = (\n",
    "        0.50*tmp[\"est_margin\"].fillna(0).rank(pct=True) +\n",
    "        0.30*tmp[\"discount_norm\"].fillna(0).rank(pct=True) +\n",
    "        0.20*tmp[\"scope_relevance\"].fillna(0).rank(pct=True)\n",
    "    )\n",
    "    tie = (tie - tie.min()) / (tie.max() - tie.min() + 1e-9)\n",
    "\n",
    "    # soft penalty for NoPromo to avoid topping unless clearly better\n",
    "    is_np = ((tmp.get(\"promo_type\").astype(str) == \"NoPromo\") | (tmp.get(\"promo_id\").astype(str) == \"__NOPROMO__\")).astype(float)\n",
    "    nopromo_penalty = 0.03 * is_np\n",
    "\n",
    "    tmp[\"final_score\"] = (\n",
    "        w[\"ptype_prob\"]*tmp[\"ptype_prob\"] +\n",
    "        w[\"ranker_score\"]*tmp[\"ranker_score\"] +\n",
    "        w[\"scope_relevance\"]*tmp[\"scope_relevance\"] +\n",
    "        w[\"est_margin\"]*tmp[\"est_margin\"] +\n",
    "        w[\"discount_norm\"]*tmp[\"discount_norm\"] +\n",
    "        w[\"is_active_now\"]*tmp[\"is_active_now\"]\n",
    "        - pen[\"type_dup_penalty\"]*tmp[\"type_dup_penalty\"]\n",
    "        - pen[\"dup_product_penalty\"]*tmp[\"dup_product_penalty\"]\n",
    "        - nopromo_penalty\n",
    "        + 0.01 * tie\n",
    "    )\n",
    "\n",
    "    # small deterministic jitter to break any remaining ties\n",
    "    if tmp[\"final_score\"].nunique() == 1:\n",
    "        j = (tmp[\"promo_id\"].astype(str).apply(lambda x: (hash(x) % 1009)/1009.0)) * 1e-4\n",
    "        tmp[\"final_score\"] = tmp[\"final_score\"] + j\n",
    "\n",
    "    return tmp.sort_values(\"final_score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "sample_tx_id = basket_feat[COL_TX].iloc[9000]\n",
    "score_event(sample_tx_id, basket_feat, ptype_model, ptype_classes, ptype_featcols, promos_df, rank_art).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "23d521cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>promo_id</th>\n",
       "      <th>promo_type</th>\n",
       "      <th>discount</th>\n",
       "      <th>product_scope</th>\n",
       "      <th>ranker_score</th>\n",
       "      <th>final_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PR0005</td>\n",
       "      <td>Buy 1 get 1</td>\n",
       "      <td>100.0</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.249258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PR0078</td>\n",
       "      <td>Buy 1 get 1</td>\n",
       "      <td>100.0</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.249258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PR0030</td>\n",
       "      <td>Buy 1 get 1</td>\n",
       "      <td>100.0</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.249258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PR0034</td>\n",
       "      <td>Buy 1 get 1</td>\n",
       "      <td>100.0</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.249258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PR0095</td>\n",
       "      <td>Buy 1 get 1</td>\n",
       "      <td>100.0</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.249258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  promo_id   promo_type  discount product_scope  ranker_score  final_score\n",
       "0   PR0005  Buy 1 get 1     100.0                         1.0     0.249258\n",
       "1   PR0078  Buy 1 get 1     100.0                         1.0     0.249258\n",
       "2   PR0030  Buy 1 get 1     100.0                         1.0     0.249258\n",
       "3   PR0034  Buy 1 get 1     100.0                         1.0     0.249258\n",
       "4   PR0095  Buy 1 get 1     100.0                         1.0     0.249258"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_tx_id = basket_feat[\"transaction_id\"].iloc[5]\n",
    "rec = score_event(sample_tx_id, basket_feat, ptype_model, ptype_classes, ptype_featcols,\n",
    "                  promos_df, rank_art, topk=2, rel_th=0.30)\n",
    "rec[['promo_id','promo_type','discount','product_scope','ranker_score','final_score']].head(5)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c60a48db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "def apply_guardrails(\n",
    "    ranked_promos: pd.DataFrame,\n",
    "    k: int = 5,\n",
    "    gap_rule_min_gap: float = 0.05,\n",
    "    min_real_promos: int = 2,\n",
    "    diversity_by: List[str] = [\"promo_type\", \"product_scope\"],\n",
    "    max_per_type: int = 2,\n",
    "    cap_nopromo: int = 1,\n",
    "    nopromo_label: str = \"NoPromo\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Enforce guardrails over a single event candidate list already scored with `final_score`.\n",
    "    Assumes columns: promo_id, promo_type, product_scope, final_score.\n",
    "    Returns top-k after rules.\n",
    "    \"\"\"\n",
    "    df = ranked_promos.copy()\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # 1) sort by final score\n",
    "    df = df.sort_values(\"final_score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # 2) cap NoPromo count\n",
    "    if cap_nopromo is not None and cap_nopromo >= 0:\n",
    "        is_np = (df[\"promo_type\"] == nopromo_label) | (df[\"promo_id\"] == \"__NOPROMO__\")\n",
    "        keep_np = df[is_np].head(cap_nopromo)\n",
    "        keep_non = df[~is_np]\n",
    "        df = pd.concat([keep_non, keep_np], ignore_index=True)\n",
    "        df = df.sort_values(\"final_score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # 3) max per type\n",
    "    if max_per_type is not None and max_per_type > 0 and \"promo_type\" in df.columns:\n",
    "        df[\"_type_rank\"] = df.groupby(\"promo_type\").cumcount()\n",
    "        df = df[df[\"_type_rank\"] < max_per_type].drop(columns=[\"_type_rank\"])  \n",
    "\n",
    "    # 4) diversity constraints: ensure no exact duplicate scopes back-to-back\n",
    "    if diversity_by:\n",
    "        seen_keys = set()\n",
    "        rows = []\n",
    "        for _, r in df.iterrows():\n",
    "            key = tuple(r.get(col, \"\") for col in diversity_by)\n",
    "            if key not in seen_keys:\n",
    "                rows.append(r)\n",
    "                seen_keys.add(key)\n",
    "            if len(rows) >= k * 3:  # keep buffer before gap rule\n",
    "                break\n",
    "        df = pd.DataFrame(rows)\n",
    "        if not df.empty:\n",
    "            df = df.sort_values(\"final_score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # 5) gap rule: keep items until score drops too much from best\n",
    "    if not df.empty:\n",
    "        best = float(df[\"final_score\"].iloc[0])\n",
    "        df = df[df[\"final_score\"] >= best - gap_rule_min_gap]\n",
    "        df = df.head(max(k, min_real_promos))\n",
    "\n",
    "    # 6) ensure minimum real promos\n",
    "    is_np = (df[\"promo_type\"] == nopromo_label) | (df[\"promo_id\"] == \"__NOPROMO__\")\n",
    "    num_real = int((~is_np).sum())\n",
    "    if num_real < min_real_promos:\n",
    "        # pull more real promos from the original list\n",
    "        src = ranked_promos.sort_values(\"final_score\", ascending=False)\n",
    "        extra = src[(~((src[\"promo_type\"] == nopromo_label) | (src[\"promo_id\"] == \"__NOPROMO__\"))) & (~src[\"promo_id\"].isin(df[\"promo_id\"]))]\n",
    "        need = min_real_promos - num_real\n",
    "        if need > 0 and not extra.empty:\n",
    "            df = pd.concat([df, extra.head(need)], ignore_index=True)\n",
    "            df = df.sort_values(\"final_score\", ascending=False).head(max(k, min_real_promos))\n",
    "\n",
    "    # final trim to k\n",
    "    df = df.sort_values(\"final_score\", ascending=False).head(k)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "de3adb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5140\\1814488970.py:52: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  non_np_per_event = recs.groupby(\"event_id\").apply(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'ndcg@3': 0.9568674437241791,\n",
       "  'ndcg@5': 0.9568674437241791,\n",
       "  'coverage': 0.992},\n",
       "     event_id     promo_id      promo_type  final_score\n",
       " 0  TX0013649  __NOPROMO__         NoPromo     0.577496\n",
       " 1  TX0013649       PR0011  Product_Coupon     0.072343\n",
       " 2  TX0013649       PR0065  Product_Coupon     0.069708\n",
       " 3  TX0002059  __NOPROMO__         NoPromo     0.568125\n",
       " 4  TX0002059       PR0011  Product_Coupon    -0.235423\n",
       " 5  TX0002059       PR0065  Product_Coupon    -0.237590\n",
       " 6  TX0010175  __NOPROMO__         NoPromo     0.578439\n",
       " 7  TX0010175       PR0011  Product_Coupon    -0.073720\n",
       " 8  TX0010175       PR0065  Product_Coupon    -0.075949\n",
       " 9  TX0004561  __NOPROMO__         NoPromo     0.567094)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch scoring + guardrails + validation\n",
    "\n",
    "import random\n",
    "\n",
    "def batch_score_with_guardrails(\n",
    "    event_ids: List,\n",
    "    basket_feats: pd.DataFrame,\n",
    "    ptype_model,\n",
    "    ptype_classes: List[str],\n",
    "    ptype_featcols: List[str],\n",
    "    promos_df: pd.DataFrame,\n",
    "    rank_art: dict,\n",
    "    k: int = 5,\n",
    "    gap: float = 0.05,\n",
    "    min_real: int = 2,\n",
    "    diversity_by: List[str] = [\"promo_type\",\"product_scope\"],\n",
    "    max_per_type: int = 2,\n",
    "    cap_nopromo: int = 1,\n",
    "    nopromo_label: str = \"NoPromo\",\n",
    ") -> Tuple[pd.DataFrame, dict]:\n",
    "    rec_rows = []\n",
    "    metrics = {\"ndcg@3\": [], \"ndcg@5\": [], \"coverage\": 0.0}\n",
    "\n",
    "    # ground truth for validation\n",
    "    # label_df from earlier cell\n",
    "    truth = label_df.set_index(COL_TX)[\"used_type\"].to_dict()\n",
    "\n",
    "    for eid in event_ids:\n",
    "        ranked = score_event(\n",
    "            eid, basket_feats, ptype_model, ptype_classes, ptype_featcols, promos_df, rank_art\n",
    "        )\n",
    "        final = apply_guardrails(\n",
    "            ranked, k=k, gap_rule_min_gap=gap, min_real_promos=min_real,\n",
    "            diversity_by=diversity_by, max_per_type=max_per_type,\n",
    "            cap_nopromo=cap_nopromo, nopromo_label=nopromo_label\n",
    "        )\n",
    "\n",
    "        # collect results\n",
    "        final = final.assign(event_id=eid)\n",
    "        rec_rows.append(final)\n",
    "\n",
    "        # NDCG vs truth: relevance=1 if promo_type equals used_type\n",
    "        used = truth.get(eid, \"NoPromo\")\n",
    "        rels = (final[\"promo_type\"].values == used).astype(int)\n",
    "        metrics[\"ndcg@3\"].append(ndcg_at_k(rels, 3))\n",
    "        metrics[\"ndcg@5\"].append(ndcg_at_k(rels, 5))\n",
    "\n",
    "    recs = pd.concat(rec_rows, ignore_index=True) if rec_rows else pd.DataFrame()\n",
    "\n",
    "    # coverage: share of events with at least one non-NoPromo recommended\n",
    "    if not recs.empty:\n",
    "        non_np_per_event = recs.groupby(\"event_id\").apply(\n",
    "            lambda g: (g[\"promo_type\"] != nopromo_label).any()\n",
    "        ).mean()\n",
    "        metrics[\"coverage\"] = float(non_np_per_event)\n",
    "    else:\n",
    "        metrics[\"coverage\"] = 0.0\n",
    "\n",
    "    metrics[\"ndcg@3\"] = float(np.mean(metrics[\"ndcg@3\"])) if metrics[\"ndcg@3\"] else 0.0\n",
    "    metrics[\"ndcg@5\"] = float(np.mean(metrics[\"ndcg@5\"])) if metrics[\"ndcg@5\"] else 0.0\n",
    "    return recs, metrics\n",
    "\n",
    "# Run on a random sample of events\n",
    "sample_events = basket_feat[COL_TX].drop_duplicates().sample(n=min(500, len(basket_feat)), random_state=SEED).tolist()\n",
    "recs, m = batch_score_with_guardrails(\n",
    "    sample_events,\n",
    "    basket_feat,\n",
    "    ptype_model,\n",
    "    ptype_classes,\n",
    "    ptype_featcols,\n",
    "    promos_df,\n",
    "    rank_art,\n",
    "    k=5,\n",
    "    gap=0.05,\n",
    "    min_real=2,\n",
    "    diversity_by=[\"promo_type\",\"product_scope\"],\n",
    "    max_per_type=2,\n",
    "    cap_nopromo=1,\n",
    ")\n",
    "\n",
    "m, recs.head(10)[[\"event_id\",\"promo_id\",\"promo_type\",\"final_score\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fb372d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra ranking metrics\n",
    "\n",
    "def precision_recall_at_k(pred_types, true_type, k=5):\n",
    "    topk = list(pred_types[:k])\n",
    "    hits = sum(t == true_type for t in topk)\n",
    "    prec = hits / max(k, 1)\n",
    "    rec = 1.0 if true_type in topk else 0.0  # single-label recall\n",
    "    return float(prec), float(rec)\n",
    "\n",
    "\n",
    "def reciprocal_rank(pred_types, true_type):\n",
    "    for i, t in enumerate(pred_types, start=1):\n",
    "        if t == true_type:\n",
    "            return float(1.0 / i)\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def average_precision(pred_types, true_type):\n",
    "    ap, hits = 0.0, 0\n",
    "    for i, t in enumerate(pred_types, start=1):\n",
    "        if t == true_type:\n",
    "            hits += 1\n",
    "            ap += hits / i\n",
    "    return float(ap / max(hits, 1)) if hits else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d64682ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttrain's ndcg@3: 0.999962\ttrain's ndcg@5: 0.999972\tvalid's ndcg@3: 0.996579\tvalid's ndcg@5: 0.997372\n",
      "Early stopping, best iteration is:\n",
      "[51]\ttrain's ndcg@3: 0.998484\ttrain's ndcg@5: 0.998946\tvalid's ndcg@3: 0.99719\tvalid's ndcg@5: 0.997908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ndcg@3': 0.821997287112932,\n",
       " 'ndcg@5': 0.821997287112932,\n",
       " 'coverage': 0.9671532846715328,\n",
       " 'precision@5': 0.18847758081334723,\n",
       " 'recall@5': 0.8331595411887383,\n",
       " 'mrr': 0.8159106708376782,\n",
       " 'map': 0.8180396246089676}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train/Test split by event_time from tx_merge3.csv and full evaluation\n",
    "\n",
    "# 1) Build event list with timestamps\n",
    "_events = basket_feat[[COL_TX, \"event_time\"]].drop_duplicates().dropna()\n",
    "_events = _events.sort_values(\"event_time\")\n",
    "cut = int(len(_events) * 0.8)\n",
    "train_events = set(_events.iloc[:cut][COL_TX].tolist())\n",
    "test_events  = set(_events.iloc[cut:][COL_TX].tolist())\n",
    "\n",
    "# 2) Rebuild rank_df restricted to train events and train a fresh ranker\n",
    "rank_df_train = rank_df[rank_df[\"event_id\"].isin(train_events)].copy()\n",
    "rank_art_tt   = train_ranker(rank_df_train)\n",
    "\n",
    "# 3) Evaluate on test events with guardrails\n",
    "truth = label_df.set_index(COL_TX)[\"used_type\"].to_dict()\n",
    "\n",
    "def eval_on_events(event_ids, k_list=(3,5), k_guard=5):\n",
    "    ndcgs = {f\"ndcg@{k}\": [] for k in k_list}\n",
    "    cover, precs, recs, mrrs, maps = [], [], [], [], []\n",
    "\n",
    "    for eid in event_ids:\n",
    "        raw = score_event(eid, basket_feat, ptype_model, ptype_classes, ptype_featcols, promos_df, rank_art_tt)\n",
    "        fin = apply_guardrails(raw, k=k_guard, gap_rule_min_gap=0.05, min_real_promos=2,\n",
    "                               diversity_by=[\"promo_type\",\"product_scope\"], max_per_type=2, cap_nopromo=1)\n",
    "        # relevance by promo_type match (single-label)\n",
    "        y_true = truth.get(eid, \"NoPromo\")\n",
    "        rels = (fin[\"promo_type\"].values == y_true).astype(int)\n",
    "        for k in k_list:\n",
    "            ndcgs[f\"ndcg@{k}\"].append(ndcg_at_k(rels, k))\n",
    "        cover.append((fin[\"promo_type\"] != \"NoPromo\").any())\n",
    "        p, r = precision_recall_at_k(fin[\"promo_type\"].values, y_true, k=k_guard)\n",
    "        precs.append(p); recs.append(r)\n",
    "        mrrs.append(reciprocal_rank(fin[\"promo_type\"].values, y_true))\n",
    "        maps.append(average_precision(fin[\"promo_type\"].values, y_true))\n",
    "\n",
    "    out = {m: float(np.mean(v)) if v else 0.0 for m, v in ndcgs.items()}\n",
    "    out.update({\n",
    "        \"coverage\": float(np.mean(cover)) if cover else 0.0,\n",
    "        f\"precision@{k_guard}\": float(np.mean(precs)) if precs else 0.0,\n",
    "        f\"recall@{k_guard}\": float(np.mean(recs)) if recs else 0.0,\n",
    "        \"mrr\": float(np.mean(mrrs)) if mrrs else 0.0,\n",
    "        \"map\": float(np.mean(maps)) if maps else 0.0,\n",
    "    })\n",
    "    return out\n",
    "\n",
    "metrics_test = eval_on_events(sorted(test_events), k_list=(3,5), k_guard=5)\n",
    "metrics_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a5f6e678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>promo_id</th>\n",
       "      <th>promo_type</th>\n",
       "      <th>final_score</th>\n",
       "      <th>ranker_score</th>\n",
       "      <th>ptype_prob</th>\n",
       "      <th>scope_relevance</th>\n",
       "      <th>est_margin</th>\n",
       "      <th>discount_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PR0005</td>\n",
       "      <td>Buy 1 get 1</td>\n",
       "      <td>0.790203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.520811</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__NOPROMO__</td>\n",
       "      <td>NoPromo</td>\n",
       "      <td>0.059535</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.358139</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      promo_id   promo_type  final_score  ranker_score  ptype_prob  \\\n",
       "0       PR0005  Buy 1 get 1     0.790203           1.0    0.520811   \n",
       "1  __NOPROMO__      NoPromo     0.059535           0.0    0.358139   \n",
       "\n",
       "   scope_relevance  est_margin  discount_norm  \n",
       "0              1.0         0.0            1.0  \n",
       "1              0.0         0.0            0.0  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx_id = basket_feat[\"transaction_id\"].iloc[6]\n",
    "rec = score_event(tx_id, basket_feat, ptype_model, ptype_classes, ptype_featcols, promos_df, rank_art)\n",
    "rec[[\"promo_id\",\"promo_type\",\"final_score\",\"ranker_score\",\"ptype_prob\",\"scope_relevance\",\"est_margin\",\"discount_norm\"]].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b24e6b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "promotion_products.csv written to Datasets\\mockup_ver2\\promotion_products.csv with shape (100, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5140\\3787613756.py:9: UserWarning: Parsing dates in %d/%m/%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  promotions_df_full = pd.read_csv(prom_path, parse_dates=[\"start_date\",\"end_date\"], dayfirst=False)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5140\\3787613756.py:9: UserWarning: Parsing dates in %d/%m/%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  promotions_df_full = pd.read_csv(prom_path, parse_dates=[\"start_date\",\"end_date\"], dayfirst=False)\n"
     ]
    }
   ],
   "source": [
    "# === Step 1: Build promotion-product mapping and export CSV ===\n",
    "from collections import defaultdict\n",
    "\n",
    "prod_path = BASE/\"products.csv\"\n",
    "prom_path = BASE/\"promotions.csv\"\n",
    "prom_tx_path = BASE/\"promotion_transactions.csv\"\n",
    "\n",
    "products_df = pd.read_csv(prod_path)\n",
    "promotions_df_full = pd.read_csv(prom_path, parse_dates=[\"start_date\",\"end_date\"], dayfirst=False)\n",
    "try:\n",
    "    promo_tx = pd.read_csv(prom_tx_path)\n",
    "except FileNotFoundError:\n",
    "    promo_tx = pd.DataFrame(columns=[\"transaction_id\",\"promo_id\",\"product_id\",\"min_qty\",\"discount_applied\"])  # safe empty\n",
    "\n",
    "# Normalize\n",
    "_products = products_df.rename(columns={\"category\": \"category\", \"brand\": \"brand\"})\n",
    "_proms = promotions_df_full.copy()\n",
    "\n",
    "# Build product_ids list per promo from historical mapping\n",
    "promo_to_products = (\n",
    "    promo_tx.groupby(\"promo_id\")[\"product_id\"].apply(lambda s: sorted(set(s.dropna().astype(str)))).to_dict()\n",
    ")\n",
    "\n",
    "# Lookup for product -> (category, brand)\n",
    "prod_lookup = _products.set_index(\"product_id\")[ [\"category\",\"brand\"] ]\n",
    "\n",
    "# Infer category/brand scopes heuristically\n",
    "category_scope = {}\n",
    "brand_scope = {}\n",
    "min_qty_map = {}\n",
    "if not promo_tx.empty:\n",
    "    if \"min_qty\" in promo_tx.columns:\n",
    "        min_qty_map = promo_tx.groupby(\"promo_id\")[\"min_qty\"].min().fillna(1).astype(int).to_dict()\n",
    "    for pid, plist in promo_to_products.items():\n",
    "        idx = [p for p in plist if p in prod_lookup.index]\n",
    "        if not idx:\n",
    "            continue\n",
    "        dfp = prod_lookup.loc[idx]\n",
    "        cat_counts = dfp[\"category\"].value_counts()\n",
    "        br_counts  = dfp[\"brand\"].value_counts()\n",
    "        if len(dfp):\n",
    "            if not cat_counts.empty and (cat_counts.iloc[0] / len(dfp) >= 0.6):\n",
    "                category_scope[pid] = [str(cat_counts.index[0])]\n",
    "            if not br_counts.empty and (br_counts.iloc[0] / len(dfp) >= 0.6):\n",
    "                brand_scope[pid] = [str(br_counts.index[0])]\n",
    "\n",
    "# Defaults\n",
    "DEFAULT_MIN_QTY = 1\n",
    "DEFAULT_MAX_DISCOUNT_PER_USER = 1000.0\n",
    "\n",
    "rows = []\n",
    "for _, pr in _proms.iterrows():\n",
    "    pid = pr.get(\"promo_id\")\n",
    "    rows.append({\n",
    "        \"promo_id\": pid,\n",
    "        \"product_id\": \",\".join(promo_to_products.get(pid, [])),\n",
    "        \"category_scope\": \",\".join(category_scope.get(pid, [])),\n",
    "        \"brand_scope\": \",\".join(brand_scope.get(pid, [])),\n",
    "        \"min_qty\": int(min_qty_map.get(pid, DEFAULT_MIN_QTY)),\n",
    "        \"max_discount_per_user\": DEFAULT_MAX_DISCOUNT_PER_USER\n",
    "    })\n",
    "\n",
    "promotion_products = pd.DataFrame(rows)\n",
    "\n",
    "# Persist\n",
    "out_path = BASE/\"promotion_products.csv\"\n",
    "promotion_products.to_csv(out_path, index=False)\n",
    "print(f\"promotion_products.csv written to {out_path} with shape {promotion_products.shape}\")\n",
    "\n",
    "# Helper: build fast lookup dicts\n",
    "_promoprod_lookup = {\n",
    "    r[\"promo_id\"]: {\n",
    "        \"product_ids\": [p for p in str(r[\"product_id\"]).split(\",\") if p and p != 'nan'],\n",
    "        \"categories\": [c for c in str(r[\"category_scope\"]).split(\",\") if c and c != 'nan'],\n",
    "        \"brands\": [b for b in str(r[\"brand_scope\"]).split(\",\") if b and b != 'nan'],\n",
    "        \"min_qty\": r[\"min_qty\"],\n",
    "        \"max_discount_per_user\": r[\"max_discount_per_user\"],\n",
    "    }\n",
    "    for _, r in promotion_products.iterrows()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a4ab337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 2: Enhanced features + precise scope relevance ===\n",
    "\n",
    "# Safe utilities using available data\n",
    "try:\n",
    "    tx_df_full = tx_merge.copy()\n",
    "except NameError:\n",
    "    tx_df_full = pd.read_csv(BASE/\"transactions.csv\")\n",
    "\n",
    "# Join with promo usage if available\n",
    "try:\n",
    "    promo_tx_full = pd.read_csv(BASE/\"promotion_transactions.csv\")\n",
    "except FileNotFoundError:\n",
    "    promo_tx_full = pd.DataFrame(columns=[\"transaction_id\",\"promo_id\",\"product_id\"])  \n",
    "\n",
    "# Build quick helper indices\n",
    "_tx_by_id = tx_df_full.groupby(\"transaction_id\")\n",
    "_promotx_by_promo = promo_tx_full.groupby(\"promo_id\") if not promo_tx_full.empty else {}\n",
    "\n",
    "\n",
    "def _get_basket_details(df_rows: pd.DataFrame) -> dict:\n",
    "    return {\n",
    "        'product_ids': df_rows['product_id'].astype(str).tolist() if 'product_id' in df_rows.columns else [],\n",
    "        'categories': df_rows.get('products.category', pd.Series([], dtype=str)).astype(str).tolist() if 'products.category' in df_rows.columns else [],\n",
    "        'brands': df_rows.get('products.brand', pd.Series([], dtype=str)).astype(str).tolist() if 'products.brand' in df_rows.columns else [],\n",
    "        'quantities': df_rows.get('qty', pd.Series([], dtype=float)).astype(float).tolist() if 'qty' in df_rows.columns else [],\n",
    "        'values': df_rows.get('_revenue', pd.Series([], dtype=float)).astype(float).tolist() if '_revenue' in df_rows.columns else (df_rows.get('price', pd.Series([], dtype=float)).astype(float).tolist() if 'price' in df_rows.columns else []),\n",
    "    }\n",
    "\n",
    "\n",
    "def get_historical_conversion(promo_id: str) -> float:\n",
    "    if isinstance(_promotx_by_promo, dict) or promo_tx_full.empty:\n",
    "        return 0.0\n",
    "    grp = _promotx_by_promo.get_group(promo_id) if promo_id in _promotx_by_promo.groups else None\n",
    "    if grp is None or grp.empty:\n",
    "        return 0.0\n",
    "    # crude estimate: unique transactions using this promo / total transactions during active days\n",
    "    used_tx = grp['transaction_id'].nunique()\n",
    "    prom_row = promotions_df_full[promotions_df_full['promo_id']==promo_id]\n",
    "    if prom_row.empty:\n",
    "        return min(1.0, used_tx / max(len(tx_df_full), 1))\n",
    "    s, e = prom_row.iloc[0]['start_date'], prom_row.iloc[0]['end_date']\n",
    "    if 'timestamp' in tx_df_full.columns and pd.notna(s) and pd.notna(e):\n",
    "        mask = (pd.to_datetime(tx_df_full['timestamp'], errors='coerce')>=s) & (pd.to_datetime(tx_df_full['timestamp'], errors='coerce')<=e)\n",
    "        denom = int(tx_df_full.loc[mask, 'transaction_id'].nunique()) or 1\n",
    "    else:\n",
    "        denom = int(tx_df_full['transaction_id'].nunique()) or 1\n",
    "    return float(used_tx/denom)\n",
    "\n",
    "\n",
    "def get_avg_basket_lift(promo_id: str) -> float:\n",
    "    # estimate: avg qty of eligible items with promo vs without (very rough)\n",
    "    if promo_tx_full.empty:\n",
    "        return 0.0\n",
    "    elig = promo_tx_full[promo_tx_full['promo_id']==promo_id]\n",
    "    if elig.empty:\n",
    "        return 0.0\n",
    "    tx_ids = elig['transaction_id'].unique().tolist()\n",
    "    q_with = tx_df_full[tx_df_full['transaction_id'].isin(tx_ids)].get('qty', pd.Series([], dtype=float)).astype(float)\n",
    "    q_all = tx_df_full.get('qty', pd.Series([], dtype=float)).astype(float)\n",
    "    if q_all.empty:\n",
    "        return 0.0\n",
    "    return float(q_with.mean() - q_all.mean())\n",
    "\n",
    "\n",
    "def get_user_promo_history(user_id, promo_id: str) -> float:\n",
    "    if user_id is None or promo_tx_full.empty:\n",
    "        return 0.0\n",
    "    if 'user_id' not in tx_df_full.columns:\n",
    "        return 0.0\n",
    "    tx_ids = tx_df_full[tx_df_full['user_id']==user_id]['transaction_id'].unique().tolist()\n",
    "    if not tx_ids:\n",
    "        return 0.0\n",
    "    used = promo_tx_full[(promo_tx_full['transaction_id'].isin(tx_ids)) & (promo_tx_full['promo_id']==promo_id)]\n",
    "    return float(min(1.0, used['transaction_id'].nunique() / max(len(tx_ids),1)))\n",
    "\n",
    "\n",
    "def calculate_eligible_revenue(basket_df: pd.DataFrame, eligible_products: list[str]) -> float:\n",
    "    if not eligible_products:\n",
    "        return 0.0\n",
    "    elig = basket_df[basket_df['product_id'].astype(str).isin(set(eligible_products))]\n",
    "    if '_revenue' in elig.columns:\n",
    "        return float(elig['_revenue'].sum())\n",
    "    if {'price','qty'}.issubset(elig.columns):\n",
    "        return float((elig['price'].fillna(0)*elig['qty'].fillna(0)).sum())\n",
    "    if 'price' in elig.columns:\n",
    "        return float(elig['price'].fillna(0).sum())\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def calculate_enhanced_features(basket_df: pd.DataFrame, basket_row: pd.Series, promotion: pd.Series, promoprod_lookup: dict) -> dict:\n",
    "    uid = basket_row.get('user_id', None)\n",
    "    pid = promotion.get('promo_id')\n",
    "    scope = promoprod_lookup.get(pid, {\"product_ids\":[],\"categories\":[],\"brands\":[],\"min_qty\":1,\"max_discount_per_user\":1000.0})\n",
    "    basket_details = _get_basket_details(basket_df)\n",
    "\n",
    "    basket_products = basket_details['product_ids']\n",
    "    eligible_products = scope['product_ids']\n",
    "\n",
    "    inter = len(set(basket_products) & set(eligible_products))\n",
    "    product_overlap_ratio = float(inter / max(len(basket_products), 1))\n",
    "\n",
    "    eligible_revenue = calculate_eligible_revenue(basket_df, eligible_products)\n",
    "    actual_discount_value = float(eligible_revenue * float(promotion.get('discount', 0) or 0) / 100.0)\n",
    "\n",
    "    conv = get_historical_conversion(pid)\n",
    "    lift = get_avg_basket_lift(pid)\n",
    "    affinity = get_user_promo_history(uid, pid)\n",
    "\n",
    "    now = pd.to_datetime(basket_row.get('event_time', pd.NaT), errors='coerce')\n",
    "    s = pd.to_datetime(promotion.get('start_date', pd.NaT), errors='coerce')\n",
    "    days_since_start = int((now - s).days) if (pd.notna(now) and pd.notna(s)) else 0\n",
    "    promotion_freshness = float(1.0 / (1 + max(days_since_start, 0)))\n",
    "\n",
    "    # simple competition proxy: count active promos of same type at this moment\n",
    "    same_type_active = 0\n",
    "    if 'promo_type' in promotions_df_full.columns:\n",
    "        t = promotion.get('promo_type')\n",
    "        if pd.notna(now):\n",
    "            active = promotions_df_full[(promotions_df_full['promo_type']==t) & (promotions_df_full['start_date']<=now) & (now<=promotions_df_full['end_date'])]\n",
    "            same_type_active = int(len(active))\n",
    "    promo_uniqueness_score = float(1.0 / (1 + same_type_active))\n",
    "\n",
    "    return {\n",
    "        'product_overlap_ratio': product_overlap_ratio,\n",
    "        'eligible_revenue': eligible_revenue,\n",
    "        'actual_discount_value': actual_discount_value,\n",
    "        'promo_conversion_rate': conv,\n",
    "        'promo_avg_basket_lift': lift,\n",
    "        'user_promo_affinity': affinity,\n",
    "        'days_since_start': days_since_start,\n",
    "        'promotion_freshness': promotion_freshness,\n",
    "        'similar_promos_active': same_type_active,\n",
    "        'promo_uniqueness_score': promo_uniqueness_score,\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_precise_scope_relevance(basket_df: pd.DataFrame, promotion: pd.Series, promoprod_lookup: dict) -> float:\n",
    "    # Detailed basket\n",
    "    bd = _get_basket_details(basket_df)\n",
    "    scope = promoprod_lookup.get(promotion.get('promo_id'), {\"product_ids\":[],\"categories\":[],\"brands\":[]})\n",
    "\n",
    "    # base relevance\n",
    "    product_match = len(set(bd['product_ids']) & set(scope.get('product_ids', [])))\n",
    "    category_match = len(set(bd['categories']) & set(scope.get('categories', [])))\n",
    "    brand_match = len(set(bd['brands']) & set(scope.get('brands', [])))\n",
    "\n",
    "    denom_p = max(len(bd['product_ids']), 1)\n",
    "    denom_c = max(len(bd['categories']), 1)\n",
    "    denom_b = max(len(bd['brands']), 1)\n",
    "\n",
    "    relevance = (\n",
    "        0.5 * (product_match / denom_p) +\n",
    "        0.3 * (category_match / denom_c) +\n",
    "        0.2 * (brand_match / denom_b)\n",
    "    )\n",
    "\n",
    "    # value weight boost\n",
    "    try:\n",
    "        values = bd['values']\n",
    "        prods = bd['product_ids']\n",
    "        val_total = float(sum(values)) or 1.0\n",
    "        value_weight = float(sum(v for i, v in enumerate(values) if prods[i] in set(scope.get('product_ids', []))))\n",
    "        value_ratio = float(value_weight / val_total)\n",
    "    except Exception:\n",
    "        value_ratio = 0.0\n",
    "\n",
    "    final_relevance = 0.7 * relevance + 0.3 * value_ratio\n",
    "    return float(max(0.0, min(1.0, final_relevance)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "52647f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 3: Redefine recall, training features, tie-breaking, and scoring v2 ===\n",
    "\n",
    "# Smart tie-breaking per requirement\n",
    "\n",
    "def apply_tiebreaking(candidates: pd.DataFrame) -> pd.DataFrame:\n",
    "    if candidates.empty:\n",
    "        return candidates\n",
    "    score_threshold = 0.001\n",
    "    df = candidates.copy()\n",
    "    df['score_bucket'] = (df['final_score'] / score_threshold).astype(int)\n",
    "    # ensure columns exist with safe defaults\n",
    "    for c in ['promo_conversion_rate','promotion_freshness','promo_uniqueness_score','est_margin']:\n",
    "        if c not in df.columns:\n",
    "            df[c] = 0.0\n",
    "    df['tiebreak_score'] = (\n",
    "        df['promo_conversion_rate'] * 0.4 +\n",
    "        df['promotion_freshness'] * 0.3 +\n",
    "        df['promo_uniqueness_score'] * 0.2 +\n",
    "        df['est_margin'] * 0.1\n",
    "    )\n",
    "    df['final_score_adjusted'] = (\n",
    "        df['final_score'] +\n",
    "        df['tiebreak_score'] * 0.01 +\n",
    "        df['promo_id'].astype(str).apply(lambda x: (hash(x) % 1000) / 1_000_000)\n",
    "    )\n",
    "    return df.sort_values('final_score_adjusted', ascending=False).drop(columns=['score_bucket'], errors='ignore')\n",
    "\n",
    "\n",
    "# Override recall to use precise scope relevance and keep type gating\n",
    "\n",
    "def recall_candidates_for_event_relaxed(basket_row: pd.Series,\n",
    "                                        promos_df: pd.DataFrame,\n",
    "                                        probs: np.ndarray,\n",
    "                                        classes: list,\n",
    "                                        topk_types: int = 2,\n",
    "                                        relevance_thresh: float = 0.30,\n",
    "                                        nopromo_label: str = \"NoPromo\") -> pd.DataFrame:\n",
    "    top_types = get_top_types(probs, classes, k=topk_types, ensure_non_nopromo=2, nopromo_label=nopromo_label)\n",
    "    now = basket_row.get(\"event_time\", pd.NaT)\n",
    "\n",
    "    # Select candidate promos by date/channel/type\n",
    "    def _elig(df, strict_online=True):\n",
    "        out = df.copy()\n",
    "        if 'start_date' in out.columns and 'end_date' in out.columns and pd.notna(now):\n",
    "            out = out[(out['start_date'] <= now) & (now <= out['end_date'])]\n",
    "        if strict_online and 'is_online' in out.columns and 'is_online' in basket_row.index:\n",
    "            out = out[out['is_online'] == int(basket_row['is_online'])]\n",
    "        return out\n",
    "\n",
    "    cand = _elig(promos_df, strict_online=True)\n",
    "    if 'promo_type' in cand.columns:\n",
    "        cand = cand[cand['promo_type'].isin(top_types)]\n",
    "\n",
    "    # Build basket rows for the transaction to compute relevance/features\n",
    "    tx_id = basket_row.get('transaction_id')\n",
    "    basket_tx_rows = tx_merge[tx_merge['transaction_id']==tx_id] if 'transaction_id' in tx_merge.columns else pd.DataFrame()\n",
    "\n",
    "    def _score_add(df_):\n",
    "        df_ = df_.copy()\n",
    "        df_['scope_relevance'] = df_.apply(lambda r: calculate_precise_scope_relevance(basket_tx_rows, r, _promoprod_lookup), axis=1)\n",
    "        # add enhanced per-promo features\n",
    "        enh = df_.apply(lambda r: pd.Series(calculate_enhanced_features(basket_tx_rows, basket_row, r, _promoprod_lookup)), axis=1)\n",
    "        for col in enh.columns:\n",
    "            df_[col] = enh[col]\n",
    "        return df_\n",
    "\n",
    "    cand = _score_add(cand)\n",
    "    out = cand[cand['scope_relevance'] >= relevance_thresh]\n",
    "\n",
    "    if out.empty:\n",
    "        cand2 = _elig(promos_df, strict_online=False)\n",
    "        if 'promo_type' in cand2.columns:\n",
    "            cand2 = cand2[cand2['promo_type'].isin(top_types)]\n",
    "        out = _score_add(cand2)\n",
    "        out = out[out['scope_relevance'] >= max(0.2, relevance_thresh*0.75)]\n",
    "\n",
    "    if out.empty:\n",
    "        cand3 = _elig(promos_df, strict_online=False)\n",
    "        out = _score_add(cand3)\n",
    "        out = out.nlargest(50, 'scope_relevance')\n",
    "\n",
    "    nopromo = pd.DataFrame([{\n",
    "        'promo_id': '__NOPROMO__', 'promo_type': nopromo_label,\n",
    "        'product_scope': '', 'est_margin': 0.0, 'scope_relevance': 0.0,\n",
    "        'product_overlap_ratio': 0.0, 'eligible_revenue': 0.0, 'actual_discount_value': 0.0,\n",
    "        'promo_conversion_rate': 0.0, 'promo_avg_basket_lift': 0.0,\n",
    "        'user_promo_affinity': 0.0, 'days_since_start': 0, 'promotion_freshness': 0.0,\n",
    "        'similar_promos_active': 0, 'promo_uniqueness_score': 0.0\n",
    "    }])\n",
    "    return pd.concat([out, nopromo], ignore_index=True).drop_duplicates(subset=['promo_id'], keep='first')\n",
    "\n",
    "\n",
    "# Upgrade training feature set and params\n",
    "\n",
    "def train_ranker(rank_df: pd.DataFrame, k_list=(3,5)):\n",
    "    base_F = [\n",
    "        'ptype_prob','scope_relevance','est_margin','discount_norm','is_active_now','days_to_end',\n",
    "        'type_dup_penalty','dup_product_penalty','is_online','order_hour','dayofweek','need_state_cluster'\n",
    "    ]\n",
    "    extra_F = [\n",
    "        'product_overlap_ratio','eligible_revenue','actual_discount_value',\n",
    "        'promo_conversion_rate','promo_avg_basket_lift','user_promo_affinity',\n",
    "        'promotion_freshness','promo_uniqueness_score'\n",
    "    ]\n",
    "    F = [f for f in base_F + extra_F if f in rank_df.columns]\n",
    "\n",
    "    ev = rank_df['event_id'].unique()\n",
    "    tr_e, va_e = train_test_split(ev, test_size=0.2, random_state=SEED)\n",
    "    tr = rank_df[rank_df['event_id'].isin(tr_e)]\n",
    "    va = rank_df[rank_df['event_id'].isin(va_e)]\n",
    "\n",
    "    def to_group(df_):\n",
    "        grp_sizes = df_.groupby('event_id').size().values\n",
    "        X = df_[F].fillna(0).values\n",
    "        y = df_['label'].values\n",
    "        return X, y, grp_sizes\n",
    "\n",
    "    if HAS_LGB:\n",
    "        Xtr, ytr, gtr = to_group(tr)\n",
    "        Xva, yva, gva = to_group(va)\n",
    "        try:\n",
    "            dtr = lgb.Dataset(Xtr, label=ytr, group=gtr)\n",
    "            dva = lgb.Dataset(Xva, label=yva, group=gva, reference=dtr)\n",
    "            params = dict(\n",
    "                objective='lambdarank',\n",
    "                metric='ndcg',\n",
    "                eval_at=[1,3,5],\n",
    "                label_gain=[0,1,3,7,15],\n",
    "                max_position=10,\n",
    "                learning_rate=0.05,\n",
    "                num_leaves=63,\n",
    "                min_data_in_leaf=50,\n",
    "                min_sum_hessian_in_leaf=5.0,\n",
    "                lambda_l1=0.1,\n",
    "                lambda_l2=0.1,\n",
    "                feature_fraction=0.85,\n",
    "                bagging_fraction=0.85,\n",
    "                bagging_freq=1,\n",
    "                verbosity=-1,\n",
    "                seed=SEED,\n",
    "            )\n",
    "            cbs = []\n",
    "            try: cbs.append(lgb.early_stopping(stopping_rounds=100))\n",
    "            except Exception: pass\n",
    "            try: cbs.append(lgb.log_evaluation(100))\n",
    "            except Exception: pass\n",
    "            try:\n",
    "                model = lgb.train(params, dtr, num_boost_round=800, valid_sets=[dtr, dva], valid_names=['train','valid'], callbacks=cbs)\n",
    "            except ValueError:\n",
    "                model = lgb.train(params, dtr, num_boost_round=800, valid_sets=[dtr, dva], valid_names=['train','valid'])\n",
    "            use_core_api = True\n",
    "        except Exception:\n",
    "            ranker = lgb.LGBMRanker(objective='lambdarank', n_estimators=800, learning_rate=0.05,\n",
    "                                    num_leaves=63, subsample=0.85, colsample_bytree=0.85, random_state=SEED)\n",
    "            try: ranker.set_params(metric='ndcg', eval_at=[1,3,5], label_gain=[0,1,3,7,15])\n",
    "            except Exception: pass\n",
    "            try:\n",
    "                ranker.fit(Xtr, ytr, group=gtr.tolist(), eval_set=[(Xva, yva)], eval_group=[gva.tolist()])\n",
    "            except TypeError:\n",
    "                ranker.fit(Xtr, ytr, group=gtr.tolist())\n",
    "            model = ranker\n",
    "            use_core_api = False\n",
    "\n",
    "        # Evaluate\n",
    "        def _predict(grp_df):\n",
    "            if use_core_api:\n",
    "                return model.predict(grp_df[F].fillna(0).values, num_iteration=getattr(model, 'best_iteration', None))\n",
    "            return model.predict(grp_df[F].fillna(0).values)\n",
    "\n",
    "        ndcgs = {f'ndcg@{k}': [] for k in k_list}\n",
    "        for eid, grp in va.groupby('event_id'):\n",
    "            s = _predict(grp)\n",
    "            grp = grp.assign(_s=s).sort_values('_s', ascending=False)\n",
    "            for k in k_list:\n",
    "                ndcgs[f'ndcg@{k}'].append(ndcg_at_k(grp['label'].values, k))\n",
    "        return {'model': model, 'feature_cols': F, 'report': {m: float(np.mean(v)) for m, v in ndcgs.items()}}\n",
    "\n",
    "    # fallback classifier\n",
    "    clf = GradientBoostingClassifier(random_state=SEED)\n",
    "    Xtr, ytr, _ = to_group(tr)\n",
    "    Xva, yva, _ = to_group(va)\n",
    "    clf.fit(Xtr, ytr)\n",
    "    ndcgs = {f'ndcg@{k}': [] for k in k_list}\n",
    "    for eid, grp in va.groupby('event_id'):\n",
    "        s = clf.predict_proba(grp[F].fillna(0).values)[:,1]\n",
    "        grp = grp.assign(_s=s).sort_values('_s', ascending=False)\n",
    "        for k in k_list:\n",
    "            ndcgs[f'ndcg@{k}'].append(ndcg_at_k(grp['label'].values, k))\n",
    "    return {'model': clf, 'feature_cols': F, 'report': {m: float(np.mean(v)) for m, v in ndcgs.items()}, 'fallback_pointwise': True}\n",
    "\n",
    "\n",
    "# Scoring v2 using the new features + tie-breaking\n",
    "\n",
    "def score_event_v2(event_tx_id,\n",
    "                   basket_feats: pd.DataFrame,\n",
    "                   ptype_model,\n",
    "                   ptype_classes,\n",
    "                   ptype_featcols,\n",
    "                   promos_df: pd.DataFrame,\n",
    "                   rank_art: dict,\n",
    "                   topk: int = TOPK_TYPES,\n",
    "                   rel_th: float = REL_TH):\n",
    "    row = basket_feats[basket_feats[COL_TX]==event_tx_id]\n",
    "    if row.empty:\n",
    "        raise ValueError('transaction_id ไม่พบใน basket_feats')\n",
    "    row = row.iloc[0]\n",
    "\n",
    "    X = encode_features_for_ptype(row, FEATURE_COLS, ptype_featcols)\n",
    "    probs = ptype_model.predict_proba(X)[0]\n",
    "    class_to_idx = {c:i for i,c in enumerate(ptype_classes)}\n",
    "\n",
    "    cands = recall_candidates_for_event_relaxed(\n",
    "        basket_row=row,\n",
    "        promos_df=promos_df,\n",
    "        probs=probs,\n",
    "        classes=ptype_classes,\n",
    "        topk_types=TOPK_TYPES,\n",
    "        relevance_thresh=rel_th,\n",
    "        nopromo_label='NoPromo'\n",
    "    )\n",
    "\n",
    "    tmp = cands.copy()\n",
    "    tmp['ptype_prob'] = tmp['promo_type'].apply(lambda t: probs[class_to_idx.get(t, class_to_idx.get('NoPromo', 0))])\n",
    "    tmp['is_online'] = int(row.get(COL_ONLINE, 0))\n",
    "    tmp['order_hour'] = int(row.get(COL_ORDER_H, 0))\n",
    "    tmp['dayofweek'] = int(row.get(COL_DOW, 0))\n",
    "    tmp['need_state_cluster'] = int(row.get('need_state_cluster', 0))\n",
    "\n",
    "    now = row.get('event_time', pd.NaT)\n",
    "    if {'start_date','end_date'}.issubset(tmp.columns) and pd.notna(now):\n",
    "        tmp['is_active_now'] = ((tmp['start_date'] <= now) & (now <= tmp['end_date'])).astype(int)\n",
    "        tmp['days_to_end'] = (tmp['end_date'] - now).dt.days.clip(lower=-365, upper=365)\n",
    "    else:\n",
    "        tmp['is_active_now'] = 1\n",
    "        tmp['days_to_end'] = 0\n",
    "\n",
    "    if 'discount' in tmp.columns:\n",
    "        tmp['discount_norm'] = pd.to_numeric(tmp['discount'], errors='coerce').fillna(0) / 100.0\n",
    "    else:\n",
    "        tmp['discount_norm'] = 0.0\n",
    "\n",
    "    tmp['type_dup_penalty'] = (tmp.groupby('promo_type')['promo_id'].transform('count') - 1).clip(lower=0).fillna(0)\n",
    "    if 'product_id' in tmp.columns:\n",
    "        tmp['dup_product_penalty'] = (tmp.groupby('product_id')['promo_id'].transform('count') - 1).clip(lower=0).fillna(0)\n",
    "    else:\n",
    "        tmp['dup_product_penalty'] = 0.0\n",
    "\n",
    "    needed = [\n",
    "        'ptype_prob','scope_relevance','est_margin','discount_norm','is_active_now','days_to_end',\n",
    "        'type_dup_penalty','dup_product_penalty','is_online','order_hour','dayofweek','need_state_cluster',\n",
    "        'product_overlap_ratio','eligible_revenue','actual_discount_value','promo_conversion_rate',\n",
    "        'promo_avg_basket_lift','user_promo_affinity','promotion_freshness','promo_uniqueness_score']\n",
    "    for c in needed:\n",
    "        if c not in tmp.columns:\n",
    "            tmp[c] = 0\n",
    "    tmp[needed] = tmp[needed].fillna(0)\n",
    "\n",
    "    F = rank_art['feature_cols']\n",
    "    mdl = rank_art['model']\n",
    "    Xr = tmp[F].fillna(0).values\n",
    "    if HAS_LGB and 'fallback_pointwise' not in rank_art:\n",
    "        s = mdl.predict(Xr, num_iteration=getattr(mdl, 'best_iteration', None))\n",
    "    else:\n",
    "        s = mdl.predict_proba(Xr)[:,1]\n",
    "\n",
    "    ptp = float(np.ptp(s))\n",
    "    tmp['ranker_score'] = (s - float(np.min(s))) / ptp if ptp > 1e-9 else s\n",
    "    if tmp['ranker_score'].nunique() == 1:\n",
    "        tb = (tmp['promo_id'].astype(str).apply(lambda x: (hash(x) % 997) / 997.0)) * 0.01\n",
    "        tmp['ranker_score'] = tmp['ranker_score'] + tb\n",
    "\n",
    "    w = {\n",
    "        'ptype_prob': 0.25,\n",
    "        'ranker_score': 0.40,\n",
    "        'scope_relevance': 0.15,\n",
    "        'est_margin': 0.05,\n",
    "        'discount_norm': 0.05,\n",
    "        'is_active_now': 0.05\n",
    "    }\n",
    "    pen = {'type_dup_penalty': 0.05, 'dup_product_penalty': 0.08}\n",
    "\n",
    "    tie = (\n",
    "        0.40*tmp['promo_conversion_rate'].rank(pct=True) +\n",
    "        0.30*tmp['promotion_freshness'].rank(pct=True) +\n",
    "        0.20*tmp['promo_uniqueness_score'].rank(pct=True) +\n",
    "        0.10*tmp['est_margin'].rank(pct=True)\n",
    "    )\n",
    "    tie = (tie - tie.min()) / (tie.max() - tie.min() + 1e-9)\n",
    "\n",
    "    is_np = ((tmp.get('promo_type').astype(str) == 'NoPromo') | (tmp.get('promo_id').astype(str) == '__NOPROMO__')).astype(float)\n",
    "    nopromo_penalty = 0.03 * is_np\n",
    "\n",
    "    tmp['final_score'] = (\n",
    "        w['ptype_prob']*tmp['ptype_prob'] +\n",
    "        w['ranker_score']*tmp['ranker_score'] +\n",
    "        w['scope_relevance']*tmp['scope_relevance'] +\n",
    "        w['est_margin']*tmp['est_margin'] +\n",
    "        w['discount_norm']*tmp['discount_norm'] +\n",
    "        w['is_active_now']*tmp['is_active_now'] -\n",
    "        pen['type_dup_penalty']*tmp['type_dup_penalty'] -\n",
    "        pen['dup_product_penalty']*tmp['dup_product_penalty'] -\n",
    "        nopromo_penalty + 0.01 * tie\n",
    "    )\n",
    "\n",
    "    ranked = apply_tiebreaking(tmp)\n",
    "    return ranked.sort_values('final_score_adjusted', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# convenience alias\n",
    "score_event = score_event_v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e12937ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pca_need' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[115], line 31\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# วัตถุเหล่านี้ควรมีอยู่แล้วในโน้ตบุ๊กเทรนของคุณ\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# ptype_model, ptype_classes, ptype_featcols\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# === 1) เซฟโมเดล / พรีโปรฯ =================================================\u001b[39;00m\n\u001b[0;32m     30\u001b[0m pkl_save(ptype_model,   ARTI\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/ptype_model.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m pkl_save(\u001b[43mpca_need\u001b[49m,      ARTI\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessors/pca_need.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m pkl_save(kmeans_need,   ARTI\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessors/kmeans_need.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m pkl_save(ranker_model,  ARTI\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/ranker_model.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pca_need' is not defined"
     ]
    }
   ],
   "source": [
    "# === PATHS (ตามโครงสร้างของคุณ) ===========================================\n",
    "from pathlib import Path\n",
    "import os, json, pickle, sys, platform, sklearn\n",
    "from datetime import datetime\n",
    "\n",
    "ROOT = Path(\".\")                               # โฟลเดอร์โปรเจกต์\n",
    "DATA = ROOT / \"Datasets\" / \"mockup_ver2\"       # ที่เก็บ CSV ตามรูป\n",
    "ARTI = ROOT / \"Notebooks\" / \"artifacts\"        # โฟลเดอร์เซฟอาร์ติแฟกต์\n",
    "\n",
    "(ARTI/\"models\").mkdir(parents=True, exist_ok=True)\n",
    "(ARTI/\"preprocessors\").mkdir(parents=True, exist_ok=True)\n",
    "(ARTI/\"data\").mkdir(parents=True, exist_ok=True)\n",
    "(ARTI/\"configs\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def pkl_save(obj, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# วัตถุเหล่านี้ควรมีอยู่แล้วในโน้ตบุ๊กเทรนของคุณ\n",
    "# ptype_model, ptype_classes, ptype_featcols\n",
    "# pca_need, kmeans_need\n",
    "# ranker_model, ranker_featcols\n",
    "# FEATURE_COLS\n",
    "# (optional) scaler_ptype\n",
    "# (optional) promotion_products  -> ถ้าคุณ build แล้ว\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# === 1) เซฟโมเดล / พรีโปรฯ =================================================\n",
    "pkl_save(ptype_model,   ARTI/\"models/ptype_model.pkl\")\n",
    "pkl_save(pca_need,      ARTI/\"preprocessors/pca_need.pkl\")\n",
    "pkl_save(kmeans_need,   ARTI/\"preprocessors/kmeans_need.pkl\")\n",
    "pkl_save(ranker_model,  ARTI/\"models/ranker_model.pkl\")\n",
    "\n",
    "try:\n",
    "    if scaler_ptype is not None:\n",
    "        pkl_save(scaler_ptype, ARTI/\"preprocessors/scaler_ptype.pkl\")\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# (ถ้าใช้ LightGBM และอยากมี native ไฟล์สำรอง)\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    if hasattr(ranker_model, \"booster_\"):\n",
    "        ranker_model.booster_.save_model(str(ARTI/\"models/ranker_model.txt\"))\n",
    "except Exception as e:\n",
    "    print(\"Skip saving LightGBM native:\", e)\n",
    "\n",
    "# === 2) เซฟ CONFIGS (คอลัมน์ที่ต้องใช้ตอน infer + การ์ดเรล) ===============\n",
    "feature_config = {\n",
    "    \"ptype_classes\": list(ptype_classes),\n",
    "    \"ptype_featcols\": list(ptype_featcols),\n",
    "    \"FEATURE_COLS\": list(FEATURE_COLS),\n",
    "    \"ranker_featcols\": list(ranker_featcols),\n",
    "}\n",
    "with open(ARTI/\"configs/feature_config.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(feature_config, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "guardrails = {\n",
    "    \"gap_rule_min_gap\": 0.05,\n",
    "    \"min_real_promos\": 2,\n",
    "    \"diversity_by\": [\"promo_type\",\"product_scope\"],\n",
    "    \"max_per_type\": 2,\n",
    "    \"cap_nopromo\": 1,\n",
    "    \"nopromo_label\": \"NoPromo\",\n",
    "    \"relevance_thresh\": 0.30,\n",
    "    \"topk_types\": 3,\n",
    "    \"min_non_nopromo\": 2,\n",
    "    \"K_final\": 5\n",
    "}\n",
    "with open(ARTI/\"configs/guardrails_config.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(guardrails, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# === 3) เซฟไฟล์ mapping โปรสินค้า (ถ้ามี) ==================================\n",
    "try:\n",
    "    promotion_products.to_csv(ARTI/\"data/promotion_products.csv\", index=False)\n",
    "except NameError:\n",
    "    # ถ้าไม่ได้สร้าง promotion_products ในโน้ตบุ๊กเทรน ก็ข้ามได้\n",
    "    pass\n",
    "\n",
    "# === 4) บันทึกเวอร์ชันไลบรารี ===============================================\n",
    "versions = {\n",
    "    \"timestamp\": datetime.utcnow().isoformat()+\"Z\",\n",
    "    \"python\": sys.version,\n",
    "    \"platform\": platform.platform(),\n",
    "    \"sklearn\": sklearn.__version__,\n",
    "}\n",
    "with open(ARTI/\"configs/versions.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(versions, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ Saved artifacts to:\", ARTI.resolve())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
